---
title: transformer
description: 你需要的就是你需要的，让我们看看什么是transformer吧
published: 2024-12-01
tags: ["NLP", "transformer"]
---
# Transformer

## NLP（自然语言处理）发展史
自然语言处理（Natural Language Processing，NLP）是一门借助计算机技术研究人类语言的科学。

大致可以分为两个阶段：


### NNLM神经网络模型



NNLM 模型的思路与统计语言模型保持一致，它通过输入词语前面的` N−1 `个词语来预测当前词。

NNLM 模型首先从词表 C 中查询得到前面` N−1` 个词语对应的词向量 $C(w_{t−n+1}),…,C(w_{t−2}),C(w_{t−1})，$然后将这些词向量拼接后输入到带有激活函数的隐藏层中，通过 Softmax 函数预测当前词语的概率。

特别地，包含所有词向量的词表矩阵 C 也是模型的参数，需要通过学习获得。因此 NNLM 模型不仅能够能够根据上文预测当前词语，同时还能够给出所有词语的词向量（Word Embedding）。


### Word2Vec

获取词向量

Word2Vec 的模型结构和 NNLM 基本一致，只是训练方法有所不同，分为 CBOW (Continuous Bag-of-Words) 和 Skip-gram 两种，

**CBOW**词袋模型
使用周围的词语 $w(t−2),w(t−1),w(t+1),w(t+2)$ 来预测当前词 $w(t)$

**Skip-gram**
它使用当前词 $w(t)$ 来预测它的周围词语

与严格按照统计语言模型结构设计的 NNLM 模型不同，Word2Vec 模型在结构上更加自由，训练目标也更多地是为获得词向量服务。特别是同时通过上文和下文来预测当前词语的 CBOW 训练方法打破了语言模型“只通过上文来预测当前词”的固定思维，为后续一系列神经网络语言模型的发展奠定了基础。


但是Word2Vec没有办法处理多义词和上下文敏感的问题，因为他是单向的！

我们常见的多义词情况，比如说：
- 我们今天买了苹果手机。
- 我们今天买了苹果派。

如果我们按照单向编码，"苹果"之前都是"我们今天买了"，从而得到的苹果的向量是一样的。但是在两个句子中的苹果含义是不同的！我们很明显没办法解决多义词问题和上下文理解的问题。




### ELMo模型(Embeddings from Language Models)

Embeddings from Language Models

为了更好解决多义词问题，研究者提出了ELMo模型
与 Word2Vec 模型只能提供静态词向量不同，ELMo 模型会根据上下文动态地调整词语的词向量。

ELMo 的最大特点是它不只是为每个词生成一个固定的向量，而是 **根据上下文生成动态的词向量**，从而能够捕捉到句子中词义的变化。

#### ELMo 的工作原理

ELMo 的核心技术基于一个 **双向 LSTM**（Bidirectional LSTM）语言模型，这是一种能够同时从左右两个方向建模上下文信息的 RNN 结构。其工作过程如下：

1. **训练双向语言模型**：
    
    - ELMo 的训练过程首先使用一个双向 LSTM 网络来学习语言模型。这个模型接受大量的无监督语料数据（例如大规模文本语料库）进行训练。
    - 双向 LSTM 包含两个 RNN 层：
        - **前向 LSTM**：从左到右读取文本。
        - **反向 LSTM**：从右到左读取文本。
    - 通过这样的方式，模型可以从上下文的前后信息中获取关于词汇的更多语义信息。
    - 对于输入文本中的词语，不仅可以得到含有句法信息的词向量、也可以得到包含语义信息的词向量，最终通过加权求和得到每一个词语最终的词向量。
1. **生成词向量**：
    
    - 一旦训练完成，ELMo 会为每个输入的词生成一个上下文相关的向量表示。与 Word2Vec 和 GloVe 等模型不同，ELMo 的词向量不仅仅依赖于该词的本身，而是基于它所在的上下文来动态调整。
    - 对于每个输入句子，ELMo 会基于双向 LSTM 的隐藏层输出生成一组词向量，这些词向量可以反映出该词在特定上下文中的含义。
3. **预训练与微调**：
    
    - ELMo 在生成词嵌入时可以与下游任务的模型进行联合训练（即微调）。例如，情感分析模型或命名实体识别模型可以在 ELMo 的词嵌入基础上进一步训练，以适应特定的任务需求。

ELMo大概是通过双向LSTM编码文本，然后得到词向量，从而处理多义词问题以及上下文理解问题。

但是ELMo模型并没有使用编码能力更强的Transformer模型，而且对于双层LSTM模型中得到的句法信息和语义信息拼接时并未得到很好的处理。

### GPT 模型(Generative Pretrained Transformer)

将 ELMo 模型中的 LSTM 更换为 Transformer 的 GPT 模型就出现了。但是 GPT 模型再次追随了 NNLM 的脚步，只通过词语的上文进行预测。

**GPT**（Generative Pretrained Transformer）是 OpenAI 提出的基于 **Transformer** 架构的生成式预训练模型，与 ELMo 的双向 LSTM 相比，GPT 模型采用了更先进的 **Transformer 解码器**（Decoder），并且是自回归（autoregressive）模型。GPT 的工作原理和 ELMo 存在一些关键的差异，特别是在训练方式、架构设计和生成任务上的应用。

#### GPT 的工作原理

GPT 的核心技术基于 **Transformer** 架构，特别是 Transformer 解码器部分。Transformer 是由 **自注意力机制**（Self-Attention）和 **前馈神经网络**（Feed-Forward Neural Networks）组成，能够高效地处理长距离依赖，并并行化训练过程。GPT 的训练过程包含了 **预训练** 和 **微调** 两个阶段。

1. **预训练阶段（Pretraining）**：
    
    - GPT 使用 **自回归语言模型**（Autoregressive Language Model）的方式进行训练。与 ELMo 的双向 LSTM 不同，GPT 是一个 **单向模型**（通常从左到右），即它通过给定的前文预测下一个词。
    - 在预训练过程中，GPT 模型利用大量的文本数据，学习如何根据上下文生成下一个最可能的词。这个过程只依赖于当前词的前文信息（左侧上下文），因此 GPT 模型的训练是 **单向的**。
    - 训练时，GPT 使用了 **Transformer 解码器**的结构，主要由 **多头自注意力机制**（Multi-Head Self Attention）和 **前馈网络**构成。通过这些机制，GPT 可以通过捕捉文本中词与词之间的依赖关系来生成高质量的文本。
2. **生成文本（Autoregressive Generation）**：
    
    - 一旦训练完成，GPT 模型就能够生成基于上下文的文本。在生成过程中，GPT 会根据输入的前文（即给定的上下文）逐词生成文本。每生成一个新词，GPT 会将它作为新的上下文输入，以此继续生成下一个词，直到生成完整的句子或段落。
    - 与 ELMo 生成静态词向量不同，GPT 在每一步生成时，会根据 **当前生成的上下文**动态地调整生成的词。
3. **微调阶段（Fine-tuning）**：
    
    - 在预训练完成之后，GPT 可以在特定的下游任务上进行微调。例如，GPT 可以用于文本生成、文本分类、问答系统等任务。在微调过程中，GPT 会根据具体任务的数据进行训练，以使得模型能够适应任务的需求。
    - 微调过程中，GPT 的生成能力得到了进一步的强化，可以根据特定的上下文进行更加准确的生成和预测。

#### GPT 的架构与工作流程

1. **单向自回归生成**：
    
    - GPT 的训练和生成过程是基于自回归的，即模型的生成是依赖于之前的上下文。模型通过预测下一个最可能的词来生成文本。
    - 在训练时，GPT 使用前面的文本来预测下一个词（或标记）。这与 ELMo 的双向 LSTM 不同，后者在训练时同时考虑了上下文的前后信息。
2. **Transformer 解码器**：
    
    - GPT 使用的是 **Transformer 解码器**，它的关键组件是 **自注意力机制**（Self-Attention）和 **前馈神经网络**。自注意力机制允许模型在生成每个词时，根据输入序列中所有其他词的重要性来调整自己的输出。
    - Transformer 模型相比传统的 RNN 或 LSTM 有更高的并行化效率，能够在更大规模的数据集上进行训练，因此具有较强的性能。
3. **上下文建模**：
    
    - 与 ELMo 的双向上下文建模不同，GPT 通过单向上下文建模来生成文本。这种单向建模使得 GPT 更加擅长文本生成任务，但在理解任务（如文本分类或问答）上可能不如 BERT 或 ELMo 那样强大，因为它缺乏双向上下文的信息。
4. **位置编码**：
    
    - 由于 Transformer 模型不具备 RNN 或 LSTM 的时序性质，GPT 需要显式地为每个输入词添加 **位置编码**（Positional Encoding）。位置编码帮助模型理解每个词在序列中的相对位置。

#### GPT 与 ELMo 的区别

- **建模方式**：
    
    - **ELMo** 是双向的，通过双向 LSTM 获取上下文信息，能够处理上下文中的词义变化，更适合理解任务。
    - **GPT** 是单向的，通过自回归生成模型来基于前文生成下一个词，更适合文本生成任务。
- **训练目标**：
    
    - **ELMo** 采用掩蔽语言模型（MLM）和双向 LSTM 网络，主要通过学习上下文中的词义变化来生成词向量。
    - **GPT** 采用自回归模型（AR），通过预测下一个词来生成文本。
- **应用场景**：
    
    - **ELMo** 更适用于文本理解任务，如文本分类、命名实体识别（NER）等。
    - **GPT** 更适用于文本生成任务，如对话系统、文本创作等。

#### 总结

GPT 是一个基于 Transformer 解码器的自回归语言模型，擅长从上下文生成文本。它通过单向（从左到右）建模上下文，能够有效地生成连贯的文本段落，并可以根据上下文进行动态的文本生成。与 ELMo 通过双向 LSTM 捕捉上下文信息不同，GPT 更加专注于基于前文生成后续文本，适用于生成任务而非理解任务。
### BERT 模型(Bidirectional Encoder Representations from Transformers)

**BERT**（Bidirectional Encoder Representations from Transformers）是由 Google 提出的革命性预训练模型，它基于 **Transformer** 架构，专门设计用于 **语言理解任务**。与 **GPT** 和 **ELMo** 等模型不同，BERT 最大的亮点是它采用了 **双向编码器**（Bidirectional Encoder），能够同时从左右两个方向建模上下文信息，这使得 BERT 在理解语言时具有更强的上下文语义感知能力。

#### BERT 的工作原理

BERT 的核心技术基于 **Transformer 编码器**（Transformer Encoder）架构，它不同于 GPT 的解码器架构，专注于理解任务。BERT 采用的是 **掩蔽语言模型**（Masked Language Model, MLM）训练方法，而非传统的自回归生成模型。

1. **预训练阶段（Pretraining）**：
    
    BERT 的预训练分为两大任务：
    
    - **掩蔽语言模型（Masked Language Model, MLM）**：
        - 在 MLM 中，BERT 随机掩蔽掉输入句子中的一些单词，并让模型根据上下文预测这些掩蔽的词。
        - 这种训练方式允许模型在两个方向（从左到右、从右到左）同时利用上下文信息来预测词汇，从而获得更深刻的语义理解。
        - 举例：在句子“**The cat sat on the [MASK]**”中，BERT 会根据上下文来预测 `[MASK]` 处的词，正确预测是 "mat"。
    - **下一句预测（Next Sentence Prediction, NSP）**：
        - 在 NSP 中，BERT 训练模型判断两句话之间的关系是否连贯。例如，给定句子 A 和句子 B，BERT 会被训练成预测 B 是否是 A 的下一个句子。
        - 通过这种任务，BERT 能够学习句子间的关联和语义关系，有助于捕捉更长程的上下文。
2. **训练细节**：
    
    - BERT 在预训练过程中使用了大量的文本语料库，例如维基百科、BookCorpus 等大规模文本数据，来学习通用的语言表示。
    - 预训练完成后，BERT 会生成每个词的 **上下文相关词向量**，这些词向量可以用于下游任务。
3. **微调阶段（Fine-tuning）**：
    
    - BERT 在预训练后，可以应用于特定的下游任务，通过 **微调** 来使模型更好地适应具体任务。
    - 微调过程中，BERT 只需要稍作调整，就能在各种任务上获得非常好的效果。微调的任务包括文本分类、命名实体识别（NER）、问答系统（如 SQuAD）、推断等。

#### BERT 的架构

BERT 使用的是 **Transformer 编码器**（Encoder），与 GPT 使用的解码器（Decoder）不同。Transformer 编码器通过自注意力机制来建模输入序列中各个词之间的依赖关系。

1. **自注意力机制**（Self-Attention）：
    
    - 自注意力机制允许模型在处理每个词时，同时考虑到该词与序列中其他所有词的关系，而不仅仅是直接邻近的词。这使得 BERT 能够捕捉到词与词之间的长程依赖关系。
2. **位置编码**：
    
    - Transformer 不具备像 RNN 或 LSTM 那样的序列处理能力，因此需要引入 **位置编码** 来为模型提供词汇的顺序信息。位置编码是将词语的位置信息嵌入到输入的词向量中，使模型能够理解每个词在句子中的相对位置。
3. **多层 Transformer 编码器**：
    
    - BERT 模型通常有多个 Transformer 编码器层（例如，BERT-base 使用 12 层，BERT-large 使用 24 层），这些编码器层通过多头自注意力机制对输入的词序列进行处理，从而学习到更深层次的语义信息。
4. **双向性**：
    
    - 与传统的语言模型不同，BERT 在训练时同时考虑了 **左右两个方向**的上下文信息。也就是说，对于每个词，BERT 在预测时能够同时利用该词前后的上下文，从而更好地理解词义。

#### BERT 的优势

1. **双向上下文建模**：
    
    - BERT 采用的是双向编码模型（通过自注意力机制），能够同时考虑前后文，捕捉到更加丰富的上下文信息。这比传统的左到右或右到左的模型（如 GPT 或 ELMo）更为强大。
2. **预训练和微调机制**：
    
    - BERT 提供了一个强大的预训练模型，用户可以在此基础上进行微调，快速适应各种具体任务。这种 **预训练-微调** 的策略大大提升了模型的迁移能力，能够以较少的数据实现较高的性能。
3. **强大的迁移能力**：
    
    - 通过预训练学习到的通用语言表示，BERT 可以在不同的任务（如问答、情感分析、命名实体识别等）中表现得非常优秀。这是因为 BERT 能够理解语言的深层语义结构和上下文关系。
4. **广泛应用**：
    
    - 由于 BERT 在语言理解任务上的优异表现，它被广泛应用于 **文本分类、问答系统、命名实体识别、自然语言推理** 等任务中，并成为了许多 NLP 系统的核心组件。

#### BERT 与其他模型的对比

- **与 ELMo 的对比**：
    
    - **ELMo** 是基于双向 LSTM 的语言模型，而 BERT 是基于 Transformer 编码器的模型。尽管 ELMo 也能处理双向上下文信息，但 Transformer 架构比 LSTM 更高效，能够处理更长距离的依赖关系。
    - ELMo 是生成式的（生成词向量），而 BERT 是 **掩蔽语言模型**（Masked Language Model），通过预训练学习通用的上下文表示。
- **与 GPT 的对比**：
    
    - **GPT** 是基于自回归模型（单向的）进行文本生成，而 BERT 是双向的，通过掩蔽语言模型来学习上下文关系。
    - GPT 主要用于文本生成任务，而 BERT 更适用于语言理解任务（如问答、分类、命名实体识别等）。

#### 总结

BERT 是基于 **Transformer 编码器** 的预训练语言模型，采用了双向的上下文建模方式，能够学习到深刻的语义表示。通过预训练和微调，BERT 在多种 NLP 任务中表现出了卓越的能力，尤其是在理解任务中，其双向上下文建模使得它比单向模型更具优势。BERT 的成功推动了自然语言处理领域的重大进展，成为了许多 NLP 应用的基石。
### 大模型(Large Language Model)

在自然语言处理（NLP）领域，随着技术的进步，许多大型预训练模型应运而生。它们通常基于 Transformer 架构，并在 BERT 的基础上进行了扩展和改进。这些模型通过训练大规模的语料库并使用更强大的计算能力，来提高 NLP 任务的性能。常见的 **大模型** 包括 RoBERTa、T5、ALBERT、DistilBERT、XLNet 等。

这些大模型主要通过以下方式增强了 BERT 的能力：

- **更大规模的预训练数据**：使用更多的文本数据进行训练，从而学习到更丰富的语义信息。
- **改进的训练目标**：引入了新的训练任务和策略，进一步提升了模型的表现。
- **更深的模型架构**：通过增加 Transformer 层数，提升了模型的表示能力。
- **优化计算效率**：通过压缩模型或采用更高效的训练策略，降低了训练和推理的计算成本。

#### 规模扩展定律（Scaling Laws）
**规模扩展定律（Scaling Laws）** 是指在机器学习和深度学习领域，随着模型规模（如参数数量）、训练数据量以及计算资源的增加，模型性能（如准确率、推理速度等）呈现出的规律性变化。这些定律为理解和设计大规模模型提供了理论基础，帮助研究人员和工程师在训练和部署大模型时做出更合适的决策。

##### **规模扩展定律的核心思想**

规模扩展定律的核心思想是，通过增加模型的参数数量、训练数据量以及计算资源，模型性能通常会持续提升，但这种提升通常不是线性的，而是呈现某种特定的规律。特别是在 NLP（自然语言处理）和计算机视觉等任务中，模型规模的增加往往能带来显著的性能提升。

在一定范围内，**规模扩展定律**指出，增加模型的规模（无论是参数数量、数据集大小还是计算资源）可以使得模型的性能不断提高，但提升的速度会逐渐减缓，通常会趋向某种 **幂律（power law）** 关系，表现为边际效应递减。

##### **规模扩展定律的关键要素**

1. **模型参数数量（Model Size）**：
    
    - 模型的参数数量直接影响模型的学习能力。大模型能够通过更多的参数拟合复杂的关系，从而在任务上表现得更好。
    - 例如，**GPT-3** 比 **GPT-2** 有更多的参数（1750 亿 vs. 15 亿），并且在许多任务中表现更好。
2. **训练数据量（Training Data）**：
    
    - 增加训练数据量可以帮助模型学习到更多样的语言模式和知识，从而提高泛化能力。
    - 比如，在大规模数据集（如 Common Crawl、Wikipedia）上训练的大模型，通常比在较小数据集上训练的模型更强大。
3. **计算资源（Compute）**：
    
    - 训练大规模模型需要巨大的计算资源，这通常涉及高性能的 GPU 或 TPU 集群，以及数周甚至数月的训练时间。
    - 计算资源的增加允许我们训练更大的模型和更复杂的算法，从而提高模型的效果。

##### **规模扩展定律的数学描述**

规模扩展定律通常用 **幂律（power law）** 或 **对数关系** 来描述，即：

$Performance∝(Model Size)^a×(Training Data)^b×(Compute)^{c}$

其中，**a**、**b** 和 **c** 是不同的指数，代表了模型大小、数据量和计算资源对性能提升的影响程度。

例如，某些研究表明，随着模型参数的增加，性能的提升会呈现幂律关系，随着模型规模扩大，性能的边际提升会逐渐减缓。

##### **Scaling Laws 典型的表现**

1. **参数数量和性能的关系**：
    - 模型的性能通常与其参数数量成正比，特别是在计算机视觉和 NLP 任务中，增加参数数量可以显著提高模型的表现。例如，GPT-3 在许多 NLP 任务中的表现远超其前身 GPT-2，就是因为其拥有更多的参数。
2. **数据量和性能的关系**：
    - 在训练深度学习模型时，更多的数据通常会提高模型的泛化能力。对于大规模模型而言，数据量的增加能够让模型更好地捕捉复杂的模式，减少过拟合的风险。
3. **计算资源与性能的关系**：
    - 大规模训练不仅依赖于更多的参数和数据量，还需要更强大的计算资源。在相同数据集和模型规模下，更多的计算资源可以加速训练过程并进一步优化模型。

##### **Scaling Laws 的应用实例**

1. **GPT-3 和其他大规模语言模型**：
    
    - **GPT-3** 证明了规模扩展定律的有效性，随着参数量的增加，性能得到了显著提升。GPT-3 的成功主要依赖于其庞大的参数（1750 亿个）以及大量的训练数据和计算资源。相比 GPT-2，GPT-3 在生成文本、理解复杂语言结构和多任务学习方面都取得了更优的表现。
2. **图像识别中的大规模模型（如 Vision Transformers）**：
    
    - 在计算机视觉领域，使用大规模的图像数据和巨型神经网络（如 Vision Transformers）来训练模型，已证明可以有效提升图像识别的性能。例如，ViT（Vision Transformer）模型展示了当训练数据和模型规模增加时，视觉任务的准确率逐渐提高。
3. **PaLM (Pathways Language Model)**：
    
    - Google 的 **PaLM** 模型采用了超大规模的训练数据（超过 5000 亿个参数）和计算资源，达到了业界领先的性能。在多个 NLP 基准任务中，PaLM 的性能超越了许多其他模型，体现了大规模训练和计算的优势。

##### **Scaling Laws 的边际效应与挑战**

尽管规模扩展定律展示了大模型的优势，但也有一些边际效应和挑战：

1. **边际收益递减**：
    
    - 随着模型和数据量的不断增加，性能提升的速度逐渐减缓。在某些任务中，增加模型的参数和训练数据可能不会带来显著的性能提升。例如，GPT-3 和 GPT-4 尽管在参数数量上相差巨大，但其性能提升的幅度逐渐减小。
2. **资源消耗与环境影响**：
    
    - 大规模模型需要巨大的计算资源和能源，这不仅导致了高昂的成本，还对环境造成了一定的负担。例如，训练一个大型语言模型可能需要数周甚至数月的高性能计算，产生大量的碳排放。
3. **模型的可解释性与泛化能力**：
    
    - 尽管大规模模型通常具有更强的性能，但它们的可解释性较差。尤其是当模型变得更复杂时，其推理过程变得更加难以理解。此外，模型的泛化能力也可能受到训练数据偏差的影响。

**规模扩展定律**展示了随着计算资源、训练数据和模型参数的增加，机器学习模型的性能通常会得到提升。然而，这种提升不是线性的，通常呈现幂律关系。虽然大规模模型在许多任务中表现卓越，但也面临着计算资源消耗、边际收益递减和环境影响等挑战。因此，在实际应用中，如何平衡计算成本和性能提升，以及如何优化模型效率，依然是当前 AI 研究的重要课题。



## GPT
**GPT** 的工作流程大体上是通过将输入文本转换为词嵌入（embedding），然后通过 **Transformer Decoder** 来进行生成的过程。虽然它是一个 **纯 Decoder** 模型，但它仍然需要一种方式来将输入文本转化为适合处理的形式，即通过 **embedding**。

具体流程可以分为以下几个步骤：

### **1. 文本的嵌入（Embedding）**

首先，输入的文本（如一句话或一段文本）需要转化为一个数字表示，这就是 **词嵌入（word embeddings）**。这一步的主要目标是将每个词或子词（token）映射到一个高维的向量空间中，以便于模型理解和处理。

- **Tokenization**：输入文本首先会被分解为更小的单元（词、子词或字符），这种过程称为分词（tokenization）。GPT 通常使用 **Byte Pair Encoding (BPE)** 或 **SentencePiece** 等方法，将词分解成更细粒度的单元，如子词（subwords）。这一步骤将文本分解为词或子词单元。
    
- **Embedding Layer**：分词后的每个单元都会通过一个 **嵌入层（Embedding Layer）** 被映射为一个固定大小的向量。这个向量是模型可以理解的数字表示，通常是一个高维向量（例如，768 维或 2048 维），每个单元的嵌入表示都能捕捉到该单元的语义信息。
    
- **Position Encoding**：由于 Transformer 本身是无序的（即它不关心单词在序列中的位置），因此 GPT 会加入 **位置编码（Position Encoding）**，以明确词语在句子中的顺序。位置编码通过对每个词嵌入添加一个特定的向量来实现，使模型能够区分词语在输入序列中的相对位置。
    

### **2. Transformer Decoder（生成过程）**

一旦文本被转换为嵌入向量，GPT 就开始使用 **Transformer Decoder** 来生成文本。

GPT 使用的 **Transformer Decoder** 是基于 **自注意力机制（Self-Attention）** 的，它与传统的 **Encoder-Decoder** 结构不同，GPT 只使用 Decoder 来进行生成。

- **自注意力机制（Self-Attention）**：自注意力机制允许模型在生成过程中考虑输入序列中所有词之间的依赖关系。每个生成的词都会与之前的所有词进行互动，以确定当前词与其他词之间的关系。对于每个词，模型计算一个 **注意力权重**，根据这些权重对上下文中的其他词进行加权求和，进而决定如何生成下一个词。
    
- **Masked Attention**：为了保证模型生成文本时的自回归性质，GPT 在每一层的自注意力机制中引入了 **masking**（掩码）机制。它只允许模型关注当前位置之前的所有词，而不能提前“偷看”未来的词，这样保证了生成过程是逐步的、递进的。通过这种方式，模型在每个时间步只能看到先前生成的词，而无法窥视未来的词，从而使得模型的生成过程符合自然语言的自回归特性。
    
- **多头注意力（Multi-Head Attention）**：在每一层的自注意力机制中，GPT 还使用了 **多头注意力**（Multi-Head Attention）机制。多头注意力允许模型并行计算不同的注意力分数，捕捉到不同的语义关系，使得模型能够关注输入的多个方面。
    
- **前馈神经网络（Feedforward Neural Networks）**：每一层的自注意力后，GPT 还会通过一个全连接的前馈神经网络对结果进行处理。这个网络由多个线性变换和激活函数组成，进一步帮助模型捕捉复杂的语义信息。
    

### **3. 生成下一个词（Autoregressive Generation）**

- **逐词生成**：GPT 是一个 **自回归** 模型，意味着它是通过逐个生成词来完成任务的。每次生成一个词后，它会将这个词作为上下文的一部分输入到模型中，继续生成下一个词。
    
- **输出**：在每个时间步，GPT 会计算出下一个词的概率分布，然后根据该分布选择一个词作为输出。这个过程是通过 **softmax** 层实现的，softmax 将生成的词的 logits 转化为概率值，并从中选择概率最高的词作为下一个词。
    

### **4. 生成结束标记**

- **结束标记（End of Sentence Token）**：在一些任务中，生成的文本可能会有结束的标记（比如 `</s>` 或其他特定的标记），模型会在生成过程中根据特定条件（如生成了一个结束标记，或者生成达到最大长度）停止。

### **总结**

GPT 是一个 **纯 Decoder** 模型，它通过以下方式生成回答：

1. **嵌入文本**：将输入文本转化为词嵌入，并加入位置编码，使得模型能够理解文本中的单词及其顺序。
    
2. **自回归生成**：使用 **Transformer Decoder** 通过自注意力机制逐步生成文本。在每个生成步骤中，模型都会根据已经生成的部分以及输入的文本来预测下一个词，并将其作为上下文继续生成下一个词。
    
3. **逐步生成**：由于自回归特性，GPT 在生成每个词时都依赖于已生成的词，从而保持生成文本的连贯性和上下文一致性。
    

尽管 GPT 只使用了 Decoder 结构，但通过强大的自注意力机制和自回归生成方式，它能够在不需要传统的 Encoder 部分的情况下，根据输入文本生成高质量的回答。


## Transformer