---
title: hugging face æŠ±æŠ±è„¸
description: å…³äºhugging faceçš„å·¥å…·å’Œä½¿ç”¨
published: 2024-11-28
tags:
  - LLM
  - huggingface
---

## ä»€ä¹ˆæ˜¯hugging faceï¼ŸğŸ¤”

## å¦‚ä½•ä½¿ç”¨ï¼ŸğŸ”§

### hugging faceæ¨¡å‹ä¸‹è½½

> é¦–å…ˆå®‰è£…ç›¸å…³åº“
>> `pip install -U huggingface_hub`
>
>
ç„¶åç¼–å†™ä¸€ä¸ªpythonæ–‡ä»¶ï¼Œå¦‚ä¸‹
```python
# è®¾ç½®ç¯å¢ƒå˜é‡

import os

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

from huggingface_hub import hf_hub_download

# ä¸¾ä¾‹
repo_id = "Duffy/Llama-820m"

file_list = [

".gitattributes",

"README.md",

"config.json",

"generation_config.json",

"xxx.py",

"pytorch_model.bin",

"special_tokens_map.json",

"tokenizer.json",

"tokenizer_config.json",

]

local_dir = "./your_model_file_dir"

for file_name in file_list:

hf_hub_download(repo_id=repo_id, filename=file_name, local_dir=local_dir)
```


## Hugging Face Hub

Hugging Face Hub æ˜¯ä¸€ä¸ªé›†æˆæ¨¡å‹ã€æ•°æ®é›†ã€å’Œå…¶ä»–æœºå™¨å­¦ä¹ èµ„æºçš„å¹³å°ï¼Œå®ƒä½¿å¾—åˆ†äº«å’Œä½¿ç”¨æ¨¡å‹å˜å¾—éå¸¸æ–¹ä¾¿ã€‚Hub æä¾›äº†æ˜“äºä½¿ç”¨çš„ APIã€GitHub é£æ ¼çš„æ¨¡å‹ç®¡ç†ï¼Œå¹¶ä¸”æ˜¯ç›®å‰è®¸å¤š NLP å’Œè®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­æœ€æµè¡Œçš„æ¨¡å‹èµ„æºã€‚

å¯ä»¥é€šè¿‡è®¿é—® [Hugging Face Hub](https://huggingface.co/models) é¡µé¢æµè§ˆå¤§é‡çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚è¿™äº›æ¨¡å‹å¯ä»¥æŒ‰ä»»åŠ¡ç±»åˆ«ã€æ¡†æ¶ï¼ˆä¾‹å¦‚ TensorFlowã€PyTorchï¼‰å’Œé¢†åŸŸï¼ˆå¦‚ NLPã€è®¡ç®—æœºè§†è§‰ï¼‰è¿›è¡Œç­›é€‰ã€‚

æ¯ä¸ªæ¨¡å‹éƒ½æœ‰è¯¦ç»†çš„æ–‡æ¡£README fileï¼Œæè¿°æ¨¡å‹çš„ç”¨é€”ã€æ€§èƒ½å’Œé™åˆ¶ã€‚

Hugging Face æä¾›äº† transformers åº“ï¼Œé€šè¿‡ç®€å•çš„å‡ è¡Œä»£ç ï¼Œä½ å¯ä»¥è½»æ¾åŠ è½½å’Œä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ã€‚

ä½¿ç”¨ AutoModelForXxx å’Œ AutoTokenizer æ¥åŠ è½½é€‚åˆä½ ä»»åŠ¡çš„æ¨¡å‹å’Œå¯¹åº”çš„ tokenizerã€‚

å¦‚æœæˆ‘å¸Œæœ›åŠ è½½ä¸€ä¸ªæ–‡æœ¬åˆ†ç±»æ¨¡å‹ï¼ˆå¦‚ BERTï¼‰ï¼š

```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer

model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased")
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
```

è¿™ä¸¤è¡Œä»£ç ä¼šè‡ªåŠ¨ä» Hugging Face Hub ä¸‹è½½ `bert-base-uncased `æ¨¡å‹åŠå…¶å¯¹åº”çš„ tokenizerï¼Œå¹¶å‡†å¤‡å¥½è¿›è¡Œåç»­æ¨ç†æˆ–å¾®è°ƒã€‚
å¦‚æœå­˜åœ¨ç½‘ç»œé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ï¼Œç„¶åæŠŠæ¨¡å‹idæ¢ä¸ºæ¨¡å‹åœ°å€ã€‚

##### ä¸Šä¼ è‡ªå·±çš„æ¨¡å‹

å¦‚æœä½ è®­ç»ƒäº†è‡ªå·±çš„æ¨¡å‹ï¼Œå¹¶å¸Œæœ›å°†å…¶ä¸Šä¼ åˆ° Hugging Face Hubï¼Œä¸ä»–äººå…±äº«æˆ–ç”¨äºåœ¨çº¿æ¨ç†ï¼Œå¯ä»¥ä½¿ç”¨ Hugging Face çš„ CLI å·¥å…·ã€‚
  
```bash
#  å…ˆå®‰è£… Hugging Face CLI å·¥å…·ï¼š
pip install huggingface_hub
# ç„¶åç™»å½• Hugging Faceï¼š
huggingface-cli login
# éœ€è¦æä¾› Hugging Face è´¦æˆ·çš„ tokenï¼Œç™»å½•åå³å¯ä¸Šä¼ æ¨¡å‹ã€‚
# ä¸Šä¼ æ¨¡å‹çš„æ–¹æ³•ï¼š
huggingface-cli upload ./path_to_model
```
  

è¿™æ ·ï¼Œä½ å¯ä»¥æŠŠè‡ªå·±è®­ç»ƒçš„æ¨¡å‹ä¸Šä¼ åˆ° Hugging Face Hubï¼Œå…¶ä»–ç”¨æˆ·ä¹Ÿèƒ½ä¸‹è½½å¹¶ä½¿ç”¨ã€‚

##### åˆ›å»ºå’Œç®¡ç†æ¨¡å‹åº“

â€¢ Hugging Face Hub ä¸ä»…ä»…æ˜¯ä¸€ä¸ªæ¨¡å‹ä»“åº“ï¼Œå®ƒä¹Ÿæ”¯æŒæ¨¡å‹ç‰ˆæœ¬ç®¡ç†ï¼Œä½ å¯ä»¥å¯¹ä¸Šä¼ çš„æ¨¡å‹è¿›è¡Œç‰ˆæœ¬ç®¡ç†å’Œæ›´æ–°ã€‚

â€¢ æ¯æ¬¡ä¸Šä¼ æ–°ç‰ˆæœ¬çš„æ¨¡å‹æ—¶ï¼ŒHugging Face ä¼šä¸ºä½ è‡ªåŠ¨ä¿å­˜ç‰ˆæœ¬ï¼Œå…è®¸ä½ æ–¹ä¾¿åœ°å›é€€åˆ°ä¹‹å‰çš„ç‰ˆæœ¬ã€‚

  
#### Transformers åº“

transformers æ˜¯ Hugging Face æä¾›çš„ä¸€ä¸ªé‡è¦åº“ï¼Œå®ƒæ¶µç›–äº†å¾ˆå¤šè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ¨¡å‹çš„å®ç°ï¼Œå¹¶æä¾›äº†ç®€æ´çš„ API æ¥åŠ è½½ã€å¾®è°ƒå’Œä½¿ç”¨è¿™äº›æ¨¡å‹ã€‚è¿™ä¸ªåº“æ”¯æŒå¤šç§ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æã€ç¿»è¯‘ã€å‘½åå®ä½“è¯†åˆ«ç­‰ã€‚

  

##### åŠ è½½é¢„è®­ç»ƒæ¨¡å‹

transformers åº“çš„æ ¸å¿ƒç‰¹æ€§ä¹‹ä¸€æ˜¯é€šè¿‡ AutoModel å’Œ AutoTokenizer ç±»ï¼Œè½»æ¾åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œå¯¹åº”çš„ tokenizerã€‚AutoModel æ˜¯ä¸€ä¸ªé€šç”¨ç±»ï¼Œå®ƒä¼šæ ¹æ®ä½ æŒ‡å®šçš„æ¨¡å‹åç§°è‡ªåŠ¨åŠ è½½åˆé€‚çš„æ¨¡å‹ã€‚

**ä½¿ç”¨æ–¹æ³•**ï¼š

â€¢ å‡è®¾ä½ æƒ³åŠ è½½ä¸€ä¸ªç”¨äºæ–‡æœ¬åˆ†ç±»çš„ BERT æ¨¡å‹ï¼š

  

from transformers import AutoModelForSequenceClassification, AutoTokenizer

  

model_name = "bert-base-uncased"Â  _# è¿™é‡Œæ˜¯æ¨¡å‹çš„åç§°_

model = AutoModelForSequenceClassification.from_pretrained(model_name)

tokenizer = AutoTokenizer.from_pretrained(model_name)

  

â€¢ AutoModelForSequenceClassification ä¼šæ ¹æ®æ¨¡å‹åç§°ä¸‹è½½é€‚åˆæ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„æ¨¡å‹ç»“æ„ã€‚

â€¢ AutoTokenizer ä¼šåŠ è½½é€‚é…æ¨¡å‹çš„ tokenizerï¼ˆåˆ†è¯å™¨ï¼‰ï¼Œå¹¶å‡†å¤‡å¥½å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œç¼–ç ã€‚

  

**2.2 è¿›è¡Œæ¨ç†ï¼ˆInferenceï¼‰**

  

æ¨ç†è¿‡ç¨‹æ˜¯æŒ‡å°†ä¸€ä¸ªè¾“å…¥ä¼ å…¥æ¨¡å‹ï¼Œå¾—åˆ°æ¨¡å‹çš„è¾“å‡ºã€‚transformers åº“æä¾›äº†éå¸¸ç®€ä¾¿çš„æ¥å£æ¥è¿›è¡Œæ¨ç†ã€‚

  

**æ­¥éª¤**ï¼š

â€¢ å‡è®¾ä½ æƒ³ä½¿ç”¨ BERT è¿›è¡Œæƒ…æ„Ÿåˆ†æï¼š

  
```python
from transformers import AutoModelForSequenceClassification, AutoTokenizer

import torch

model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased")

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

  

# ç¼–ç è¾“å…¥æ–‡æœ¬

inputs = tokenizer("I love Hugging Face!", return_tensors="pt")

# æ¨ç†
with torch.no_grad():

Â  Â  logits = model(**inputs).logits

  

_# è·å–é¢„æµ‹ç»“æœ_

predicted_class = logits.argmax().item()

print(f"Predicted class: {predicted_class}")
```
  

â€¢ åœ¨è¿™æ®µä»£ç ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡ tokenizer ç¼–ç äº†è¾“å…¥æ–‡æœ¬ I love Hugging Face!ï¼Œç„¶åå°†å…¶ä¼ é€’ç»™æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œæœ€åä»æ¨¡å‹è¾“å‡ºä¸­è·å–é¢„æµ‹çš„ç±»æ ‡ç­¾ã€‚

  

**2.3 Fine-tuningï¼ˆå¾®è°ƒï¼‰**

  

å¾®è°ƒæ˜¯æŒ‡å°†é¢„è®­ç»ƒæ¨¡å‹æ ¹æ®è‡ªå·±çš„ä»»åŠ¡å’Œæ•°æ®è¿›è¡Œè¿›ä¸€æ­¥è®­ç»ƒã€‚åœ¨ transformers ä¸­ï¼Œå¾®è°ƒè¿‡ç¨‹å¯ä»¥é€šè¿‡ Trainer API ç®€åŒ–ã€‚Trainer æä¾›äº†å¾ˆå¤šåŠŸèƒ½ï¼ŒåŒ…æ‹¬è®­ç»ƒè¿‡ç¨‹ç®¡ç†ã€è¯„ä¼°ã€æ—¥å¿—è®°å½•ç­‰ã€‚

  

**æ­¥éª¤**ï¼š

â€¢ å‡è®¾ä½ æœ‰ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡æ•°æ®é›†ï¼Œä½ å¯ä»¥ä½¿ç”¨ Trainer æ¥å¾®è°ƒ BERT æ¨¡å‹ï¼š

  

from transformers import Trainer, TrainingArguments

  

_# è®¾ç½®è®­ç»ƒå‚æ•°_

training_args = TrainingArguments(

Â  Â  output_dir="./results",Â  Â  Â  Â  Â  _# æ¨¡å‹è¾“å‡ºæ–‡ä»¶å¤¹_

Â  Â  evaluation_strategy="epoch", Â  Â  _# æ¯ä¸ª epoch åè¯„ä¼°_

Â  Â  per_device_train_batch_size=8, Â  _# æ¯ä¸ªè®¾å¤‡çš„è®­ç»ƒæ‰¹æ¬¡å¤§å°_

Â  Â  per_device_eval_batch_size=8,Â  Â  _# æ¯ä¸ªè®¾å¤‡çš„è¯„ä¼°æ‰¹æ¬¡å¤§å°_

Â  Â  logging_dir="./logs",Â  Â  Â  Â  Â  Â  _# æ—¥å¿—æ–‡ä»¶å¤¹_

)

  

trainer = Trainer(

Â  Â  model=model, Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  _# ä½ çš„æ¨¡å‹_

Â  Â  args=training_args,Â  Â  Â  Â  Â  Â  Â  Â  _# è®­ç»ƒå‚æ•°_

Â  Â  train_dataset=train_dataset, Â  Â  Â  _# è®­ç»ƒæ•°æ®é›†_

Â  Â  eval_dataset=eval_dataset, Â  Â  Â  Â  _# éªŒè¯æ•°æ®é›†_

)

  

trainer.train()

  

â€¢ åœ¨è¿™ä¸ªä»£ç ä¸­ï¼ŒTrainer ä¼šç®¡ç†æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬ä»æ•°æ®åŠ è½½åˆ°æ¨¡å‹è®­ç»ƒï¼Œå¹¶åœ¨æ¯ä¸ª epoch åè¿›è¡Œè¯„ä¼°ã€‚ä½ åªéœ€è¦æŒ‡å®šè®­ç»ƒå‚æ•°å’Œæ•°æ®é›†ã€‚

  
å¥½çš„ï¼æˆ‘ä»¬ç»§ç»­æ·±å…¥ä»‹ç» Hugging Face çš„å…¶ä»–ä¸»è¦åŠŸèƒ½ï¼ŒåŒ…æ‹¬ **Datasets åº“**ã€**Tokenizers åº“**ã€**Inference API** ç­‰ã€‚

  

**3. Datasets åº“**

  

Hugging Face çš„ datasets åº“æä¾›äº†ä¸€ä¸ªç®€æ´çš„æ¥å£æ¥è®¿é—®å’Œå¤„ç†å„ç§å…¬å…±æ•°æ®é›†ï¼Œå¸®åŠ©å¼€å‘è€…å¿«é€ŸåŠ è½½å’Œä½¿ç”¨æ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ã€‚è¿™ä¸ªåº“åŒ…æ‹¬äº†è®¸å¤šå¸¸ç”¨çš„ NLP æ•°æ®é›†ï¼Œæ¯”å¦‚ GLUEã€SQuADã€IMDB ç­‰ï¼Œä¹Ÿæ”¯æŒç”¨æˆ·ä¸Šä¼ è‡ªå·±çš„æ•°æ®é›†ã€‚

  

**3.1 åŠ è½½å…¬å…±æ•°æ®é›†**

  

ä½ å¯ä»¥é€šè¿‡ load_dataset å‡½æ•°ç›´æ¥åŠ è½½ Hugging Face Hub ä¸Šçš„å„ç§æ•°æ®é›†ã€‚è¿™ä¸ªåº“æ”¯æŒå¾ˆå¤šæ ¼å¼çš„æ•°æ®é›†ï¼Œæ¯”å¦‚ CSVã€JSONã€æ–‡æœ¬æ–‡ä»¶ç­‰ï¼Œä¹Ÿæ”¯æŒæ ¹æ®ä»»åŠ¡è¿›è¡Œé¢„å¤„ç†ã€‚

  

**ä½¿ç”¨æ–¹æ³•**ï¼š

â€¢ å‡è®¾ä½ è¦åŠ è½½ IMDB æ•°æ®é›†ï¼š

  

from datasets import load_dataset

  

dataset = load_dataset("imdb")

print(dataset)

  

è¿™æ®µä»£ç ä¼šè‡ªåŠ¨ä» Hugging Face Hub ä¸‹è½½å¹¶åŠ è½½ IMDB æ•°æ®é›†ã€‚load_dataset ä¼šè¿”å›ä¸€ä¸ªå­—å…¸å¯¹è±¡ï¼ŒåŒ…å«äº†è®­ç»ƒé›†ã€éªŒè¯é›†ç­‰ä¸åŒçš„åˆ†åŒºã€‚

  

â€¢ ä½ å¯ä»¥æŸ¥çœ‹æ•°æ®é›†çš„å†…å®¹ï¼š

  

print(dataset['train'][0])Â  _# æŸ¥çœ‹è®­ç»ƒé›†çš„ç¬¬ä¸€ä¸ªæ ·æœ¬_

  

  

  

**3.2 åŠ è½½è‡ªå®šä¹‰æ•°æ®é›†**

  

å¦‚æœä½ æœ‰è‡ªå·±çš„æ•°æ®é›†ï¼Œæ¯”å¦‚ CSV æ–‡ä»¶ï¼Œå¯ä»¥é€šè¿‡ Dataset ç±»å°†å…¶è½¬æ¢ä¸º Hugging Face æ ¼å¼ï¼Œä»¥ä¾¿äºå¤„ç†å’Œä½¿ç”¨ã€‚

  

**ä½¿ç”¨æ–¹æ³•**ï¼š

â€¢ å‡è®¾ä½ æœ‰ä¸€ä¸ªåŒ…å«æ–‡æœ¬å’Œæ ‡ç­¾çš„ CSV æ–‡ä»¶ï¼Œä½ å¯ä»¥å°†å…¶åŠ è½½ä¸º Hugging Face æ•°æ®é›†ï¼š

  

from datasets import Dataset

import pandas as pd

  

_# åŠ è½½ CSV æ–‡ä»¶ä¸º pandas DataFrame_

df = pd.read_csv("your_dataset.csv")

  

_# å°† DataFrame è½¬æ¢ä¸º Hugging Face Dataset æ ¼å¼_

dataset = Dataset.from_pandas(df)

print(dataset)

  

  

â€¢ å¦‚æœä½ çš„æ•°æ®é›†æ˜¯å­˜å‚¨åœ¨ JSON æ–‡ä»¶ä¸­ï¼Œä¹Ÿå¯ä»¥é€šè¿‡ç±»ä¼¼çš„æ–¹æ³•åŠ è½½ï¼š

  

dataset = load_dataset("json", data_files="your_dataset.json")

  

  

  

**3.3 æ•°æ®é›†æ“ä½œ**

  

Hugging Face çš„ datasets åº“æ”¯æŒå¯¹æ•°æ®é›†è¿›è¡Œå„ç§æ“ä½œï¼Œå¦‚ç­›é€‰ã€æ‹†åˆ†ã€æ‰¹å¤„ç†ç­‰ã€‚

â€¢ **æ•°æ®é›†æ‹†åˆ†**ï¼š

å¦‚æœä½ å¸Œæœ›å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼Œå¯ä»¥ä½¿ç”¨ train_test_split æ–¹æ³•ï¼š

  

dataset = load_dataset("imdb")

train_test = dataset["train"].train_test_split(test_size=0.2)

print(train_test)

  

  

â€¢ **è¿‡æ»¤æ•°æ®**ï¼š

å¦‚æœä½ æƒ³åŸºäºç‰¹å®šæ¡ä»¶ç­›é€‰æ•°æ®é›†ä¸­çš„æ ·æœ¬ï¼š

  

dataset_filtered = dataset.filter(lambda example: example['label'] == 1)

print(dataset_filtered)

  

  

â€¢ **æ˜ å°„å‡½æ•°**ï¼š

ä½ å¯ä»¥å¯¹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†æ“ä½œï¼Œæ¯”å¦‚æ–‡æœ¬æ¸…æ´—æˆ–åˆ†è¯ï¼š

  

def preprocess_function(examples):

Â  Â  return tokenizer(examples["text"], padding="max_length", truncation=True)

  

dataset = dataset.map(preprocess_function, batched=True)

  

  

  

**3.4 ä¿å­˜å’Œå…±äº«æ•°æ®é›†**

  

ä½ å¯ä»¥å°†å¤„ç†å¥½çš„æ•°æ®é›†ä¿å­˜ä¸ºæœ¬åœ°æ–‡ä»¶ï¼Œå¹¶ä¸Šä¼ åˆ° Hugging Face Hub è¿›è¡Œåˆ†äº«ã€‚

  

**ä¿å­˜ä¸º CSV æ–‡ä»¶**ï¼š

  

dataset.to_csv("processed_dataset.csv")

  

**ä¸Šä¼ åˆ° Hugging Face Hub**ï¼š

â€¢ åœ¨ Hugging Face Hub ä¸Šåˆ›å»ºä¸€ä¸ªæ–°çš„æ•°æ®é›†é¡µé¢å¹¶ä¸Šä¼ è‡ªå·±çš„æ•°æ®é›†ã€‚

  

**4. Tokenizers åº“**

  

Hugging Face çš„ tokenizers åº“æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„å·¥å…·ï¼Œç”¨äºå¤„ç†æ–‡æœ¬çš„åˆ†è¯å’Œç¼–ç ã€‚å®ƒæ”¯æŒå„ç§åˆ†è¯æ–¹æ³•ï¼Œå¦‚ BPEï¼ˆByte Pair Encodingï¼‰ã€WordPieceã€SentencePiece ç­‰ï¼Œå¯ä»¥å¸®åŠ©ä½ é«˜æ•ˆåœ°å¤„ç†æ–‡æœ¬æ•°æ®ã€‚

  

**4.1 åŠ è½½å’Œä½¿ç”¨ Tokenizer**

  

Hugging Face æä¾›äº†å¤šç§é¢„è®­ç»ƒçš„ tokenizerï¼Œä½ å¯ä»¥ä½¿ç”¨ AutoTokenizer ç±»æ¥åŠ è½½é€‚é…ä¸åŒæ¨¡å‹çš„åˆ†è¯å™¨ã€‚

  

**ä½¿ç”¨æ–¹æ³•**ï¼š

â€¢ å‡è®¾ä½ æƒ³ä½¿ç”¨ BERT æ¨¡å‹çš„ tokenizerï¼š

  

from transformers import AutoTokenizer

  

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

encoding = tokenizer("Hello, Hugging Face!")

print(encoding)

  

è¿™ä¼šå°†è¾“å…¥æ–‡æœ¬ "Hello, Hugging Face!" è½¬æ¢ä¸ºæ¨¡å‹èƒ½å¤Ÿç†è§£çš„ token æ ¼å¼ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªåŒ…å« token IDs çš„å­—å…¸ã€‚

  

**4.2 è‡ªå®šä¹‰ Tokenizer**

  

å¦‚æœä½ æƒ³è‡ªå®šä¹‰ä¸€ä¸ª tokenizerï¼Œå¯ä»¥ä½¿ç”¨ tokenizers åº“æä¾›çš„ä½çº§ APIã€‚ä½ å¯ä»¥é€‰æ‹©ä¸åŒçš„åˆ†è¯ç®—æ³•ï¼ˆå¦‚ BPEã€WordPiece ç­‰ï¼‰ï¼Œå¹¶æ ¹æ®è‡ªå·±çš„è¯­æ–™åº“è®­ç»ƒä¸€ä¸ªæ–°çš„ tokenizerã€‚

  

**åˆ›å»ºè‡ªå®šä¹‰ BPE Tokenizer**ï¼š

  

from tokenizers import Tokenizer, models, pre_tokenizers, trainers

  

_# åˆ›å»ºä¸€ä¸ªç©ºçš„ BPE æ¨¡å‹_

tokenizer = Tokenizer(models.BPE())

  

_# è®¾ç½®åˆ†è¯å™¨çš„é¢„å¤„ç†æ­¥éª¤_

tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()

  

_# åˆ›å»ºä¸€ä¸ªè®­ç»ƒå™¨ï¼Œè®¾ç½®ç‰¹æ®Š token_

trainer = trainers.BpeTrainer(special_tokens=["[UNK]", "[CLS]", "[SEP]"])

  

_# è®­ç»ƒ tokenizer_

tokenizer.train_from_file("your_data.txt", trainer=trainer)

  

_# ä¿å­˜æ¨¡å‹_

tokenizer.save("custom_tokenizer.json")

  

â€¢ è¿™æ®µä»£ç ä¼šä½¿ç”¨ä½ çš„æ•°æ®ï¼ˆyour_data.txtï¼‰è®­ç»ƒä¸€ä¸ª BPE åˆ†è¯å™¨ï¼Œå¹¶ä¿å­˜ä¸º custom_tokenizer.json æ–‡ä»¶ã€‚

  

**4.3 Tokenizer çš„é«˜çº§åŠŸèƒ½**

â€¢ **åˆ†è¯**ï¼š

  

tokens = tokenizer.encode("Hello, Hugging Face!")

print(tokens.tokens)Â  _# è¾“å‡º token åˆ—è¡¨_

  

  

â€¢ **è§£ç **ï¼š

  

decoded_text = tokenizer.decode(tokens.ids)

print(decoded_text)Â  _# è¾“å‡ºåŸå§‹æ–‡æœ¬_

  

  

â€¢ **ç¼–ç å’Œè§£ç æ‰¹é‡æ•°æ®**ï¼š

ä½ å¯ä»¥å¯¹å¤šä¸ªæ–‡æœ¬æ ·æœ¬è¿›è¡Œæ‰¹é‡ç¼–ç å’Œè§£ç ï¼š

  

texts = ["Hello, Hugging Face!", "I love transformers!"]

encodings = tokenizer(texts, padding=True, truncation=True, return_tensors="pt")

print(encodings)

  

**5. Inference API (æ¨ç† API)**

  

Hugging Face æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„æ¨ç† APIï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ç®€å•çš„ HTTP è¯·æ±‚å°†æ•°æ®ä¼ ç»™åœ¨çº¿æ¨¡å‹ï¼Œè·å–æ¨ç†ç»“æœã€‚è¿™å¯¹äºæ²¡æœ‰è‡ªå·±è®­ç»ƒæ¨¡å‹çš„ç”¨æˆ·ï¼Œæˆ–è€…å¸Œæœ›å¿«é€Ÿéƒ¨ç½²æ¨¡å‹çš„ç”¨æˆ·éå¸¸æœ‰ç”¨ã€‚

  

**5.1 é€šè¿‡ Transformers åº“ä½¿ç”¨æ¨ç† API**

  

Hugging Face æä¾›äº†ä¸€ä¸ªç®€ä¾¿çš„ pipeline APIï¼Œç”¨äºå¿«é€Ÿè¿›è¡Œæ¨ç†ã€‚ä½ åªéœ€è¦ä¼ å…¥æ¨¡å‹çš„åç§°å’Œä»»åŠ¡ç±»å‹ï¼Œpipeline ä¼šè‡ªåŠ¨é€‰æ‹©åˆé€‚çš„æ¨¡å‹å’Œ tokenizerï¼Œå¹¶è¿›è¡Œæ¨ç†ã€‚

  

**ä½¿ç”¨æ–¹æ³•**ï¼š

â€¢ å‡è®¾ä½ æƒ³è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼š

  

from transformers import pipeline

  

generator = pipeline("text-generation", model="gpt2")

result = generator("Once upon a time", max_length=50)

print(result)

  

  

â€¢ ä½ å¯ä»¥æŒ‡å®šå…¶ä»–ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ç­‰ï¼š

  

classifier = pipeline("sentiment-analysis")

print(classifier("I love Hugging Face!"))

  

  

  

**5.2 é€šè¿‡ HTTP è°ƒç”¨æ¨ç† API**

  

å¦‚æœä½ ä¸æƒ³ç›´æ¥ä½¿ç”¨ transformers åº“ï¼Œä¹Ÿå¯ä»¥é€šè¿‡ HTTP è¯·æ±‚ç›´æ¥è°ƒç”¨ Hugging Face æä¾›çš„ APIã€‚è¿™å¯¹äºå¼€å‘ Web æœåŠ¡æˆ–é›†æˆå…¶ä»–ç³»ç»Ÿéå¸¸æœ‰ç”¨ã€‚

  

**æ­¥éª¤**ï¼š

1. åœ¨ Hugging Face Hub ä¸Šæ‰¾åˆ°ä½ éœ€è¦çš„æ¨¡å‹ï¼Œå¹¶è·å– API Tokenã€‚

2. ä½¿ç”¨ HTTP è¯·æ±‚è°ƒç”¨æ¨¡å‹ APIã€‚

  

**ç¤ºä¾‹**ï¼š

â€¢ ä½¿ç”¨ curl è°ƒç”¨æ¨ç† APIï¼š

  

curl -X POST https://api-inference.huggingface.co/models/gpt2 \

Â Â  Â  -H "Authorization: Bearer YOUR_API_TOKEN" \

Â Â  Â  -d '{"inputs": "Once upon a time"}'

  

  

â€¢ ä½ ä¹Ÿå¯ä»¥åœ¨ Python ä¸­ä½¿ç”¨ requests å‘é€ POST è¯·æ±‚ï¼š

  

import requests

  

headers = {"Authorization": "Bearer YOUR_API_TOKEN"}

data = {"inputs": "Once upon a time"}

response = requests.post("https://api-inference.huggingface.co/models/gpt2", headers=headers, json=data)

print(response.json())

  

  

  

**5.3 æ¨ç† API çš„é«˜çº§åŠŸèƒ½**

â€¢ **æ‰¹é‡æ¨ç†**ï¼šä½ å¯ä»¥ä¸€æ¬¡æ€§å‘é€å¤šä¸ªè¾“å…¥è¿›è¡Œæ‰¹é‡æ¨ç†ã€‚

â€¢ **è‡ªå®šä¹‰å‚æ•°**ï¼šä½ å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´æ¨ç†å‚æ•°ï¼Œä¾‹å¦‚ç”Ÿæˆçš„æ–‡æœ¬é•¿åº¦ã€æ¸©åº¦ã€top-k é‡‡æ ·ç­‰ã€‚


**6. Accelerate åº“**

accelerate æ˜¯ Hugging Face æä¾›çš„ä¸€ä¸ªç”¨äºåŠ é€Ÿæ¨¡å‹è®­ç»ƒå’Œæ¨ç†çš„åº“ï¼Œç‰¹åˆ«é€‚ç”¨äºå¤§è§„æ¨¡è®­ç»ƒä»»åŠ¡ã€‚å®ƒç®€åŒ–äº†åœ¨å¤šè®¾å¤‡ï¼ˆå¦‚ GPU å’Œ TPUï¼‰ä¸Šå¹¶è¡Œè®­ç»ƒå’Œåˆ†å¸ƒå¼è®­ç»ƒçš„å¤æ‚æ€§ã€‚ä½ å¯ä»¥é€šè¿‡å®ƒæ¥è½»æ¾åœ°åœ¨å¤šä¸ªè®¾å¤‡ä¹‹é—´åˆ†é…ä»»åŠ¡ï¼Œæé«˜è®¡ç®—æ•ˆç‡ã€‚

  

**6.1 å®‰è£… Accelerate**

  

é¦–å…ˆï¼Œä½ éœ€è¦å®‰è£… accelerate åº“ï¼š

  

pip install accelerate

  

**6.2 ç®€åŒ–å¤šè®¾å¤‡è®­ç»ƒ**

  

accelerate å¯ä»¥å¸®åŠ©ä½ è‡ªåŠ¨ç®¡ç†å¤š GPU æˆ–å¤šæœºå™¨è®­ç»ƒï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡ç®€å•çš„å‘½ä»¤è¿è¡Œæ¨¡å‹è®­ç»ƒã€‚å®ƒæ”¯æŒå¤šç§è®¾å¤‡ç±»å‹ï¼ŒåŒ…æ‹¬ CPUã€GPU å’Œ TPUã€‚

  

**æ­¥éª¤**ï¼š

1. åœ¨è®­ç»ƒè„šæœ¬ä¸­ï¼Œé¦–å…ˆå¯¼å…¥ Accelerator ç±»ï¼š

  

from accelerate import Accelerator

  

  

2. åˆ›å»º Accelerator å®ä¾‹å¹¶è®¾ç½®è®­ç»ƒè¿‡ç¨‹ï¼š

  

accelerator = Accelerator()

  

  

3. ä½¿ç”¨ accelerator.prepare() æ¥è‡ªåŠ¨å‡†å¤‡æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œæ•°æ®åŠ è½½å™¨ï¼š

  

model, optimizer, train_dataloader = accelerator.prepare(model, optimizer, train_dataloader)

  

  

4. ç„¶åï¼Œä½ å¯ä»¥åƒå¹³å¸¸ä¸€æ ·è¿›è¡Œè®­ç»ƒï¼Œaccelerate ä¼šè‡ªåŠ¨å¤„ç†å¤šè®¾å¤‡çš„åŒæ­¥å’Œè°ƒåº¦ã€‚

  

**ç¤ºä¾‹**ï¼šä½¿ç”¨ accelerate è¿›è¡Œç®€å•çš„è®­ç»ƒ

  

from accelerate import Accelerator

from transformers import AutoModelForSequenceClassification, AutoTokenizer, AdamW

from datasets import load_dataset

  

_# åˆå§‹åŒ– Accelerator_

accelerator = Accelerator()

  

_# åŠ è½½æ•°æ®é›†å’Œæ¨¡å‹_

dataset = load_dataset("imdb")

model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased")

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

  

_# å‡†å¤‡æ•°æ®_

train_dataset = dataset["train"].map(lambda x: tokenizer(x["text"], padding=True, truncation=True), batched=True)

train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=8)

  

_# åˆå§‹åŒ–ä¼˜åŒ–å™¨_

optimizer = AdamW(model.parameters(), lr=5e-5)

  

_# å‡†å¤‡æ‰€æœ‰å†…å®¹_

model, optimizer, train_dataloader = accelerator.prepare(model, optimizer, train_dataloader)

  

_# è®­ç»ƒè¿‡ç¨‹_

for epoch in range(3):

Â  Â  model.train()

Â  Â  for batch in train_dataloader:

Â  Â  Â  Â  optimizer.zero_grad()

Â  Â  Â  Â  inputs = {key: batch[key].to(accelerator.device) for key in batch}

Â  Â  Â  Â  labels = inputs.pop("label")

Â  Â  Â  Â  outputs = model(**inputs, labels=labels)

Â  Â  Â  Â  loss = outputs.loss

Â  Â  Â  Â  accelerator.backward(loss)

Â  Â  Â  Â  optimizer.step()

Â  Â  print(f"Epoch {epoch} completed")

  

é€šè¿‡ä½¿ç”¨ accelerateï¼Œä½ å¯ä»¥è½»æ¾åœ°åœ¨å¤šè®¾å¤‡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä¸éœ€è¦æ‰‹åŠ¨ç¼–å†™å¤æ‚çš„åˆ†å¸ƒå¼è®­ç»ƒä»£ç ã€‚accelerate å¤„ç†äº†æ•°æ®å¹¶è¡Œã€æ¢¯åº¦ç´¯ç§¯ç­‰æ–¹é¢çš„å†…å®¹ã€‚

  

**6.3 å¹¶è¡Œæ¨ç†**

  

accelerate è¿˜æ”¯æŒæ¨ç†æ—¶çš„å¤šè®¾å¤‡åŠ é€Ÿã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æœ‰å¤šä¸ª GPUï¼Œå¹¶å¸Œæœ›å°†æ¨ç†è´Ÿè½½åˆ†é…åˆ°å¤šä¸ªè®¾å¤‡ä¸Šï¼Œå¯ä»¥ä½¿ç”¨ accelerate æ¥åŠ é€Ÿæ¨ç†ã€‚

  

from accelerate import Accelerator

from transformers import AutoModelForSequenceClassification, AutoTokenizer

  

accelerator = Accelerator()

  

model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased")

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

  

_# å‡†å¤‡æ¨¡å‹å’Œ tokenizer_

model, tokenizer = accelerator.prepare(model, tokenizer)

  

_# æ¨ç†_

inputs = tokenizer("Hello, Hugging Face!", return_tensors="pt")

inputs = {key: value.to(accelerator.device) for key, value in inputs.items()}

outputs = model(**inputs)

  

**7. Hugging Face Spaces**

  

Hugging Face Spaces æ˜¯ä¸€ä¸ªç”¨äºæ‰˜ç®¡å’Œåˆ†äº«æœºå™¨å­¦ä¹ æ¨¡å‹å’Œåº”ç”¨çš„å¹³å°ã€‚ä½ å¯ä»¥åœ¨ Hugging Face Spaces ä¸Šåˆ›å»ºã€éƒ¨ç½²å’Œåˆ†äº«ä½ çš„æœºå™¨å­¦ä¹ åº”ç”¨ï¼Œæ”¯æŒé€šè¿‡ Streamlitã€Gradio ç­‰åº“å¿«é€Ÿæ­å»ºäº¤äº’å¼ç•Œé¢ã€‚

  

**7.1 åˆ›å»º Hugging Face Spaces**

  

Hugging Face Spaces å…è®¸ç”¨æˆ·é€šè¿‡ç®€å•çš„ä»£ç å’Œç•Œé¢ï¼Œå¿«é€Ÿæ„å»º Web åº”ç”¨æ¥å±•ç¤ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ä½ å¯ä»¥é€‰æ‹©ä½¿ç”¨ Gradio æˆ– Streamlit æ¥æ„å»ºåº”ç”¨ç•Œé¢ã€‚

  

**æ­¥éª¤**ï¼š

1. ç™»å½• Hugging Face è´¦æˆ·å¹¶åˆ›å»ºä¸€ä¸ª Space é¡µé¢ã€‚

2. åœ¨ Space ä¸­åˆ›å»ºä¸€ä¸ªåº”ç”¨ï¼Œå¯ä»¥é€‰æ‹© Gradio æˆ– Streamlit æ¡†æ¶ã€‚

  

**ç¤ºä¾‹ï¼šä½¿ç”¨ Gradio åˆ›å»ºäº¤äº’å¼ç•Œé¢**ï¼š

  

pip install gradio

  

ç„¶åï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„ Gradio åº”ç”¨æ¥å±•ç¤ºæ¨¡å‹ï¼š

  

import gradio as gr

from transformers import pipeline

  

_# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹_

classifier = pipeline("sentiment-analysis")

  

_# åˆ›å»º Gradio ç•Œé¢_

def predict(text):

Â  Â  return classifier(text)

  

_# è®¾ç½®ç•Œé¢_

interface = gr.Interface(fn=predict, inputs="text", outputs="json")

  

_# å¯åŠ¨åº”ç”¨_

interface.launch()

  

è¿™æ®µä»£ç åˆ›å»ºäº†ä¸€ä¸ªç®€å•çš„æ–‡æœ¬åˆ†ç±»åº”ç”¨ï¼Œç”¨æˆ·å¯ä»¥è¾“å…¥æ–‡æœ¬ï¼Œåº”ç”¨ä¼šè¿”å›è¯¥æ–‡æœ¬çš„æƒ…æ„Ÿåˆ†æç»“æœã€‚

  

**7.2 ä½¿ç”¨ Streamlit åˆ›å»ºäº¤äº’å¼ç•Œé¢**

  

Streamlit ä¹Ÿæ˜¯ä¸€ä¸ªéå¸¸æµè¡Œçš„ Python åº“ï¼Œç”¨äºå¿«é€Ÿåˆ›å»º Web åº”ç”¨ã€‚ä½ å¯ä»¥åœ¨ Hugging Face Spaces ä¸­ä½¿ç”¨ Streamlit æ„å»ºåº”ç”¨ã€‚

  

pip install streamlit

  

ç„¶åï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„åº”ç”¨ï¼š

  

import streamlit as st

from transformers import pipeline

  

_# åŠ è½½æ¨¡å‹_

classifier = pipeline("sentiment-analysis")

  

_# åˆ›å»º Streamlit åº”ç”¨_

st.title("Sentiment Analysis")

text_input = st.text_area("Enter text:")

  

if text_input:

Â  Â  result = classifier(text_input)

Â  Â  st.write(result)

  

ç„¶åï¼Œé€šè¿‡ streamlit run å¯åŠ¨åº”ç”¨ï¼š

  

streamlit run app.py

  

è¿™æ ·ä½ å°±å¯ä»¥é€šè¿‡ Streamlit åˆ›å»ºä¸€ä¸ªç®€å•çš„åº”ç”¨ï¼Œç”¨æˆ·è¾“å…¥æ–‡æœ¬åå³å¯çœ‹åˆ°æƒ…æ„Ÿåˆ†æçš„ç»“æœã€‚

  

**7.3 åˆ†äº«å’Œéƒ¨ç½²åº”ç”¨**

  

ä¸€æ—¦ä½ åˆ›å»ºäº† Hugging Face Spaceï¼Œä½ å¯ä»¥åˆ†äº«å®ƒçš„é“¾æ¥ç»™å…¶ä»–äººï¼Œè®©ä»–ä»¬ä¹Ÿèƒ½è®¿é—®å’Œä½¿ç”¨ä½ çš„åº”ç”¨ã€‚Hugging Face ä¼šä¸ºæ¯ä¸ª Space æä¾›ä¸€ä¸ªç‹¬ç«‹çš„ URLï¼Œä½ å¯ä»¥å°†å…¶åµŒå…¥åˆ°æ–‡æ¡£ã€åšå®¢ç­‰åœ°æ–¹ã€‚

â€¢ åœ¨ Space é¡µé¢ä¸Šï¼Œä½ å¯ä»¥é€‰æ‹©å…¬å¼€æˆ–ç§æœ‰ä½ çš„åº”ç”¨ã€‚å¦‚æœä½ é€‰æ‹©å…¬å¼€ï¼Œå…¶ä»–äººä¹Ÿå¯ä»¥è®¿é—®å¹¶ä½¿ç”¨è¿™ä¸ªåº”ç”¨ã€‚

â€¢ ä½ ä¹Ÿå¯ä»¥åœ¨åº”ç”¨ä¸­ä¸Šä¼ è‡ªå·±çš„æ•°æ®æˆ–æ¨¡å‹æ–‡ä»¶ï¼Œä½¿å¾—åº”ç”¨æ›´åŠ ä¸°å¯Œå’Œä¸ªæ€§åŒ–ã€‚

  

**8. Hugging Face æ–‡æ¡£å’Œç¤¾åŒºæ”¯æŒ**

  

Hugging Face æä¾›äº†ä¸°å¯Œçš„æ–‡æ¡£å’Œç¤¾åŒºæ”¯æŒï¼Œå¸®åŠ©å¼€å‘è€…è§£å†³é—®é¢˜å¹¶å­¦ä¹ å¦‚ä½•ä½¿ç”¨ Hugging Face çš„å·¥å…·ã€‚

  

**8.1 å®˜æ–¹æ–‡æ¡£**

â€¢ [Hugging Face å®˜æ–¹æ–‡æ¡£](https://huggingface.co/docs)ï¼šåŒ…å«äº†ä»æ¨¡å‹åŠ è½½ã€è®­ç»ƒåˆ°éƒ¨ç½²ç­‰å„ä¸ªæ–¹é¢çš„è¯¦ç»†è¯´æ˜ï¼Œæ¶µç›–äº†å„ç±» API å’ŒåŠŸèƒ½çš„ä½¿ç”¨ç¤ºä¾‹ã€‚

  

**8.2 Hugging Face è®ºå›**

â€¢ [Hugging Face è®ºå›](https://discuss.huggingface.co/)ï¼šä½ å¯ä»¥åœ¨è¿™ä¸ªè®ºå›ä¸Šå‘ç¤¾åŒºæé—®ï¼Œåˆ†äº«ç»éªŒï¼Œè®¨è®ºæœ€æ–°çš„ç ”ç©¶æˆæœã€‚è®ºå›ä¸­æœ‰è®¸å¤šæ¥è‡ªä¸åŒé¢†åŸŸçš„ä¸“å®¶ï¼Œä»–ä»¬ä¼šå›ç­”ä½ çš„é—®é¢˜ï¼Œæä¾›å¸®åŠ©ã€‚

  

**8.3 Hugging Face Discord**

â€¢ Hugging Face è¿˜æä¾›äº†ä¸€ä¸ª [Discord é¢‘é“](https://discord.gg/huggingface)ï¼Œä½ å¯ä»¥åœ¨è¿™é‡Œä¸å…¶ä»–å¼€å‘è€…å’Œç ”ç©¶äººå‘˜å®æ—¶äº¤æµã€‚

  

**æ€»ç»“**

  

æˆ‘ä»¬å·²ç»è¯¦ç»†æ¢è®¨äº† Hugging Face çš„è®¸å¤šåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š

1. **Accelerate åº“**ï¼šå¸®åŠ©åœ¨å¤šä¸ªè®¾å¤‡ä¸ŠåŠ é€Ÿè®­ç»ƒå’Œæ¨ç†ï¼Œç®€åŒ–åˆ†å¸ƒå¼è®­ç»ƒã€‚

2. **Hugging Face Spaces**ï¼šç”¨äºåˆ›å»ºå’Œéƒ¨ç½²æœºå™¨å­¦ä¹ åº”ç”¨ï¼Œæ”¯æŒ Gradio å’Œ Streamlit ç•Œé¢ï¼Œæ–¹ä¾¿å±•ç¤ºå’Œåˆ†äº«æ¨¡å‹ã€‚

3. **æ–‡æ¡£å’Œç¤¾åŒºæ”¯æŒ**ï¼šæä¾›ä¸°å¯Œçš„æ–‡æ¡£ã€è®ºå›å’Œå®æ—¶äº¤æµå¹³å°ï¼Œå¸®åŠ©å¼€å‘è€…è§£å†³é—®é¢˜ã€‚

  

é€šè¿‡ä½¿ç”¨ Hugging Face æä¾›çš„å·¥å…·å’Œå¹³å°ï¼Œä½ å¯ä»¥é«˜æ•ˆåœ°æ„å»ºã€è®­ç»ƒã€éƒ¨ç½²å’Œåˆ†äº«æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚å¸Œæœ›è¿™äº›å†…å®¹èƒ½å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œå¦‚æœæœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦è¿›ä¸€æ­¥çš„ç¤ºä¾‹ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼