---
title: LoRA微调
description: LoRA微调是什么？如何进行？
published: 2024-11-11
tags:
  - LLM
  - finetuning
---
现在的大模型动辄几十、上百亿参数，如果我们想要对模型进行微调，放到本地一般而言很少有足够的显存。

大模型微调的几种方法：
- 低秩微调LoRA


  

**LoRA（Low-Rank Adaptation）简介：高效的深度学习模型微调方法**

  

在深度学习领域，尤其是大规模预训练模型（如BERT、GPT系列等）的微调过程中，传统的微调方法通常需要修改模型的所有参数。这样做虽然可以获得较好的性能，但代价非常高，特别是在面对非常大的预训练模型时。LoRA（Low-Rank Adaptation）作为一种新的微调技术，提出了一种更加高效且参数量更少的方法来进行微调。

  

**什么是LoRA？**

  

LoRA是“低秩适应”技术的缩写，其核心思想是在进行模型微调时，通过将权重矩阵分解为低秩矩阵来减少可调参数的数量。这使得我们在不改变原始模型结构的情况下，能够以更少的计算和存储开销，快速适应下游任务。

  

具体来说，LoRA通过在神经网络中的每一层加入一个低秩适应矩阵，这些矩阵只会在微调过程中进行更新，而原始的预训练参数保持不变。

  

**LoRA的原理**

  

在传统的神经网络中，参数矩阵通常是一个高维矩阵。例如，在一个线性层中，假设权重矩阵是 ，其中 和 分别是输入和输出的维度。

  

LoRA的关键想法是将这个高维矩阵 分解为两个低秩矩阵 和 ，即：

  

  

其中， 是我们在微调过程中需要学习的低秩适应矩阵。通过这种方式，只有 和 两个矩阵需要更新，且它们的秩通常设置为较小的值（例如，秩为 ）。这样可以显著减少需要训练的参数数量，同时保留原始模型的表示能力。

  

**为什么LoRA有效？**

1. **减少计算开销**：由于LoRA只需要更新较小的低秩矩阵，避免了对整个模型参数的更新，因此计算资源消耗大大降低，尤其是在大规模预训练模型上。

2. **适用于大模型**：大规模预训练模型通常包含数亿甚至数十亿个参数，直接微调这些模型往往需要巨大的计算和存储资源。LoRA通过限制可调参数的数量，使得在有限资源下仍然能够有效地进行微调。

3. **保持预训练知识**：LoRA不会修改原始的预训练权重，只是引入了少量的适应性变化。因此，模型能够保留从大规模数据集学到的知识，同时针对具体任务进行微调。

  

**LoRA的实现**

  

假设我们有一个现成的神经网络模型（例如BERT或GPT），而我们想要进行LoRA微调。LoRA的实现步骤如下：

1. **选择适合微调的层**：首先，我们需要选择模型中哪些层需要进行LoRA微调。通常会选择自注意力层（Attention）和前馈神经网络（FeedForward）层。

2. **插入低秩适应矩阵**：在选定的层中，我们将原始权重矩阵 替换为 ，其中 和 是低秩矩阵。

3. **训练低秩矩阵**：在微调过程中，我们只更新低秩矩阵 和 ，而保持原始模型权重不变。

4. **评估性能**：在完成微调后，我们评估模型在下游任务上的性能，看看LoRA是否能够有效地提高模型在特定任务上的表现。

  

**LoRA的优点**

• **参数效率高**：LoRA只需要更新少量参数，通常比传统微调方法的参数更新量要小得多，尤其是在大规模预训练模型中。

• **节省计算资源**：LoRA的低秩适应矩阵可以显著减少训练和推理时的计算开销。

• **简洁易用**：LoRA的实现相对简单，可以作为一个插件式的微调方法，适用于多种不同的神经网络架构。

  

**LoRA的应用场景**

  

LoRA非常适合用于以下几种场景：

1. **大规模预训练模型微调**：在处理诸如BERT、GPT等大型模型时，LoRA能够有效减少微调时的计算资源消耗。

2. **低资源环境下的迁移学习**：如果资源有限（例如显存不足），LoRA能够帮助在不牺牲性能的前提下进行迁移学习。

3. **模型压缩**：LoRA也可以用于模型压缩，尤其是在需要在嵌入式设备或移动设备上运行大模型时，LoRA可以通过减少训练参数的数量来降低模型的存储需求。

  

**示例代码**

  

以下是一个简单的Python示例，展示了如何在PyTorch中实现LoRA微调：

```python
import torch

import torch.nn as nn

import torch.optim as optim

  

class LoRALayer(nn.Module):

    def __init__(self, input_dim, output_dim, rank):

        super(LoRALayer, self).__init__()

        self.input_dim = input_dim

        self.output_dim = output_dim

        self.rank = rank

  

        # 初始化低秩矩阵 A 和 B

        self.A = nn.Parameter(torch.randn(input_dim, rank))

        self.B = nn.Parameter(torch.randn(rank, output_dim))

  

    def forward(self, x):

        _# 计算低秩适应矩阵的贡献_

        delta_W = torch.mm(self.A, self.B)

        return torch.matmul(x, delta_W)

  

_# 模型层的使用示例_

input_dim = 768  _# 输入维度_

output_dim = 256  _# 输出维度_

rank = 8  _# 低秩矩阵的秩_

  

_# 创建LoRA层_

lora_layer = LoRALayer(input_dim, output_dim, rank)

  

# 假设输入是一个batch的样本_

x = torch.randn(32, input_dim)  _# 假设batch size为32_
# 前向传播_
output = lora_layer(x)
```
  

**总结**

  

LoRA（Low-Rank Adaptation）作为一种高效的模型微调方法，能够在保持大规模预训练模型强大能力的同时，大幅度减少微调时的计算和存储开销。它通过将模型的权重矩阵分解为低秩矩阵，只更新这些低秩矩阵，而不需要改变原始模型的参数，从而实现了资源高效的迁移学习。

  

这种方法在处理大规模预训练模型、低资源环境中的迁移学习、以及模型压缩等任务中具有显著的优势。

  

希望这篇文章对你理解LoRA有帮助！如果有任何问题或需要进一步的示例，欢迎随时提问。