<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>hugging face æŠ±æŠ±è„¸ Â· Duffy</title><meta name="description" content="ä»€ä¹ˆæ˜¯hugging faceï¼ŸğŸ¤”å¦‚ä½•ä½¿ç”¨ï¼ŸğŸ”§hugging faceæ¨¡å‹ä¸‹è½½
é¦–å…ˆå®‰è£…ç›¸å…³åº“

pip install -U huggingface_hub


ç„¶åç¼–å†™ä¸€ä¸ªpythonæ–‡ä»¶ï¼Œå¦‚ä¸‹
123456789101112131415161718192021222324252627282"><meta name="og:description" content="ä»€ä¹ˆæ˜¯hugging faceï¼ŸğŸ¤”å¦‚ä½•ä½¿ç”¨ï¼ŸğŸ”§hugging faceæ¨¡å‹ä¸‹è½½
é¦–å…ˆå®‰è£…ç›¸å…³åº“

pip install -U huggingface_hub


ç„¶åç¼–å†™ä¸€ä¸ªpythonæ–‡ä»¶ï¼Œå¦‚ä¸‹
123456789101112131415161718192021222324252627282"><meta name="twitter:site" content="Duffy"><meta name="twitter:title" content="hugging face æŠ±æŠ±è„¸"><meta name="twitter:card" content="summary"><meta name="keywords" content=""><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/bootstrap.min.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="/css/style.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><meta name="generator" content="Hexo 7.3.0"><link rel="stylesheet" href="/css/prism.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><div class="container" id="stage"><div class="row"><div class="col-sm-3 col-xs-12 side-container invisible" id="side-bar"><div class="vertical-text site-title"><h3 class="site-title-small" tabindex="-1"><a class="a-title" href="/">Keep going</a></h3><h1 class="site-title-large" tabindex="-1"><a class="a-title" href="/">æ€è€ƒï½œè®°å½•</a></h1><!--h6(onclick="triggerSiteNav()") Trigger--></div><br class="visible-lg visible-md visible-sm"><div class="site-title-links" id="site-nav"><ul><li><a href="/">Home</a></li><li><a href="/archives">Archive</a></li><li><a href="/tags">Tags</a></li><li><a href="/about/about.html"></a></li><li><a href="/about/index.html">about</a></li><li><a href="/about/heureux.html"></a></li><li class="soc"><a href="https://github.com/heureux831" target="_blank" rel="noopener noreferrer" aria-label="Github"><i class="fa fa-github">&nbsp;</i></a><a href="http://example.com/atom.xml" target="_blank" rel="noopener noreferrer" aria-label="RSS"><i class="fa fa-rss">&nbsp;</i></a></li></ul><div class="visible-lg visible-md visible-sm site-nav-footer"><br class="site-nav-footer-br"><footer><p>&copy;&nbsp;2025&nbsp;<a target="_blank" href="http://example.com" rel="noopener noreferrer">Kai</a></p><p>Theme&nbsp;<a target="_blank" href="https://github.com/SumiMakito/hexo-theme-typography" rel="noopener noreferrer">Typography</a>&nbsp;by&nbsp;<a target="_blank" href="https://www.keep.moe" rel="noopener noreferrer">Makito</a></p><p>Proudly published with&nbsp;<a target="_blank" href="https://hexo.io" rel="noopener noreferrer">Hexo</a></p></footer></div></div></div><div class="col-sm-9 col-xs-12 main-container invisible" id="main-container"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post-container"><p class="post-title"><a>hugging face æŠ±æŠ±è„¸</a></p><p class="post-meta"><span class="date meta-item">Posted at&nbsp;2024-11-28</span><span class="meta-item"><i class="fa fa-tag"></i><span>&nbsp;</span><a class="a-tag" href="/tags/LLM/" title="LLM">LLM</a><span>&nbsp;</span><a class="a-tag" href="/tags/huggingface/" title="huggingface">huggingface</a><span>&nbsp;</span></span></p><p class="post-abstract"><h2 id="ä»€ä¹ˆæ˜¯hugging-faceï¼ŸğŸ¤”"><a href="#ä»€ä¹ˆæ˜¯hugging-faceï¼ŸğŸ¤”" class="headerlink" title="ä»€ä¹ˆæ˜¯hugging faceï¼ŸğŸ¤”"></a>ä»€ä¹ˆæ˜¯hugging faceï¼ŸğŸ¤”</h2><h2 id="å¦‚ä½•ä½¿ç”¨ï¼ŸğŸ”§"><a href="#å¦‚ä½•ä½¿ç”¨ï¼ŸğŸ”§" class="headerlink" title="å¦‚ä½•ä½¿ç”¨ï¼ŸğŸ”§"></a>å¦‚ä½•ä½¿ç”¨ï¼ŸğŸ”§</h2><h3 id="hugging-faceæ¨¡å‹ä¸‹è½½"><a href="#hugging-faceæ¨¡å‹ä¸‹è½½" class="headerlink" title="hugging faceæ¨¡å‹ä¸‹è½½"></a>hugging faceæ¨¡å‹ä¸‹è½½</h3><blockquote>
<p>é¦–å…ˆå®‰è£…ç›¸å…³åº“</p>
<blockquote>
<p><code>pip install -U huggingface_hub</code></p>
</blockquote>
</blockquote>
<p>ç„¶åç¼–å†™ä¸€ä¸ªpythonæ–‡ä»¶ï¼Œå¦‚ä¸‹</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># è®¾ç½®ç¯å¢ƒå˜é‡</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">os.environ[<span class="string">&quot;HF_ENDPOINT&quot;</span>] = <span class="string">&quot;https://hf-mirror.com&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> huggingface_hub <span class="keyword">import</span> hf_hub_download</span><br><span class="line"></span><br><span class="line"><span class="comment"># ä¸¾ä¾‹</span></span><br><span class="line">repo_id = <span class="string">&quot;Duffy/Llama-820m&quot;</span></span><br><span class="line"></span><br><span class="line">file_list = [</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;.gitattributes&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;README.md&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;config.json&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;generation_config.json&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;xxx.py&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;pytorch_model.bin&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;special_tokens_map.json&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;tokenizer.json&quot;</span>,</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;tokenizer_config.json&quot;</span>,</span><br><span class="line"></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">local_dir = <span class="string">&quot;./your_model_file_dir&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> file_name <span class="keyword">in</span> file_list:</span><br><span class="line"></span><br><span class="line">hf_hub_download(repo_id=repo_id, filename=file_name, local_dir=local_dir)</span><br></pre></td></tr></table></figure>


<h2 id="Hugging-Face-Hub"><a href="#Hugging-Face-Hub" class="headerlink" title="Hugging Face Hub"></a>Hugging Face Hub</h2><p>Hugging Face Hub æ˜¯ä¸€ä¸ªé›†æˆæ¨¡å‹ã€æ•°æ®é›†ã€å’Œå…¶ä»–æœºå™¨å­¦ä¹ èµ„æºçš„å¹³å°ï¼Œå®ƒä½¿å¾—åˆ†äº«å’Œä½¿ç”¨æ¨¡å‹å˜å¾—éå¸¸æ–¹ä¾¿ã€‚Hub æä¾›äº†æ˜“äºä½¿ç”¨çš„ APIã€GitHub é£æ ¼çš„æ¨¡å‹ç®¡ç†ï¼Œå¹¶ä¸”æ˜¯ç›®å‰è®¸å¤š NLP å’Œè®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­æœ€æµè¡Œçš„æ¨¡å‹èµ„æºã€‚</p>
<p>å¯ä»¥é€šè¿‡è®¿é—® <a target="_blank" rel="noopener" href="https://huggingface.co/models">Hugging Face Hub</a> é¡µé¢æµè§ˆå¤§é‡çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚è¿™äº›æ¨¡å‹å¯ä»¥æŒ‰ä»»åŠ¡ç±»åˆ«ã€æ¡†æ¶ï¼ˆä¾‹å¦‚ TensorFlowã€PyTorchï¼‰å’Œé¢†åŸŸï¼ˆå¦‚ NLPã€è®¡ç®—æœºè§†è§‰ï¼‰è¿›è¡Œç­›é€‰ã€‚</p>
<p>æ¯ä¸ªæ¨¡å‹éƒ½æœ‰è¯¦ç»†çš„æ–‡æ¡£README fileï¼Œæè¿°æ¨¡å‹çš„ç”¨é€”ã€æ€§èƒ½å’Œé™åˆ¶ã€‚</p>
<p>Hugging Face æä¾›äº† transformers åº“ï¼Œé€šè¿‡ç®€å•çš„å‡ è¡Œä»£ç ï¼Œä½ å¯ä»¥è½»æ¾åŠ è½½å’Œä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ã€‚</p>
<p>ä½¿ç”¨ AutoModelForXxx å’Œ AutoTokenizer æ¥åŠ è½½é€‚åˆä½ ä»»åŠ¡çš„æ¨¡å‹å’Œå¯¹åº”çš„ tokenizerã€‚</p>
<p>å¦‚æœæˆ‘å¸Œæœ›åŠ è½½ä¸€ä¸ªæ–‡æœ¬åˆ†ç±»æ¨¡å‹ï¼ˆå¦‚ BERTï¼‰ï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification, AutoTokenizer</span><br><span class="line"></span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(<span class="string">&quot;bert-base-uncased&quot;</span>)</span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-uncased&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>è¿™ä¸¤è¡Œä»£ç ä¼šè‡ªåŠ¨ä» Hugging Face Hub ä¸‹è½½ <code>bert-base-uncased </code>æ¨¡å‹åŠå…¶å¯¹åº”çš„ tokenizerï¼Œå¹¶å‡†å¤‡å¥½è¿›è¡Œåç»­æ¨ç†æˆ–å¾®è°ƒã€‚<br>å¦‚æœå­˜åœ¨ç½‘ç»œé—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ï¼Œç„¶åæŠŠæ¨¡å‹idæ¢ä¸ºæ¨¡å‹åœ°å€ã€‚</p>
<h5 id="ä¸Šä¼ è‡ªå·±çš„æ¨¡å‹"><a href="#ä¸Šä¼ è‡ªå·±çš„æ¨¡å‹" class="headerlink" title="ä¸Šä¼ è‡ªå·±çš„æ¨¡å‹"></a>ä¸Šä¼ è‡ªå·±çš„æ¨¡å‹</h5><p>å¦‚æœä½ è®­ç»ƒäº†è‡ªå·±çš„æ¨¡å‹ï¼Œå¹¶å¸Œæœ›å°†å…¶ä¸Šä¼ åˆ° Hugging Face Hubï¼Œä¸ä»–äººå…±äº«æˆ–ç”¨äºåœ¨çº¿æ¨ç†ï¼Œå¯ä»¥ä½¿ç”¨ Hugging Face çš„ CLI å·¥å…·ã€‚</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#  å…ˆå®‰è£… Hugging Face CLI å·¥å…·ï¼š</span></span><br><span class="line">pip install huggingface_hub</span><br><span class="line"><span class="comment"># ç„¶åç™»å½• Hugging Faceï¼š</span></span><br><span class="line">huggingface-cli login</span><br><span class="line"><span class="comment"># éœ€è¦æä¾› Hugging Face è´¦æˆ·çš„ tokenï¼Œç™»å½•åå³å¯ä¸Šä¼ æ¨¡å‹ã€‚</span></span><br><span class="line"><span class="comment"># ä¸Šä¼ æ¨¡å‹çš„æ–¹æ³•ï¼š</span></span><br><span class="line">huggingface-cli upload ./path_to_model</span><br></pre></td></tr></table></figure>
<p>è¿™æ ·ï¼Œä½ å¯ä»¥æŠŠè‡ªå·±è®­ç»ƒçš„æ¨¡å‹ä¸Šä¼ åˆ° Hugging Face Hubï¼Œå…¶ä»–ç”¨æˆ·ä¹Ÿèƒ½ä¸‹è½½å¹¶ä½¿ç”¨ã€‚</p>
<h5 id="åˆ›å»ºå’Œç®¡ç†æ¨¡å‹åº“"><a href="#åˆ›å»ºå’Œç®¡ç†æ¨¡å‹åº“" class="headerlink" title="åˆ›å»ºå’Œç®¡ç†æ¨¡å‹åº“"></a>åˆ›å»ºå’Œç®¡ç†æ¨¡å‹åº“</h5><p>â€¢ Hugging Face Hub ä¸ä»…ä»…æ˜¯ä¸€ä¸ªæ¨¡å‹ä»“åº“ï¼Œå®ƒä¹Ÿæ”¯æŒæ¨¡å‹ç‰ˆæœ¬ç®¡ç†ï¼Œä½ å¯ä»¥å¯¹ä¸Šä¼ çš„æ¨¡å‹è¿›è¡Œç‰ˆæœ¬ç®¡ç†å’Œæ›´æ–°ã€‚</p>
<p>â€¢ æ¯æ¬¡ä¸Šä¼ æ–°ç‰ˆæœ¬çš„æ¨¡å‹æ—¶ï¼ŒHugging Face ä¼šä¸ºä½ è‡ªåŠ¨ä¿å­˜ç‰ˆæœ¬ï¼Œå…è®¸ä½ æ–¹ä¾¿åœ°å›é€€åˆ°ä¹‹å‰çš„ç‰ˆæœ¬ã€‚</p>
<h4 id="Transformers-åº“"><a href="#Transformers-åº“" class="headerlink" title="Transformers åº“"></a>Transformers åº“</h4><p>transformers æ˜¯ Hugging Face æä¾›çš„ä¸€ä¸ªé‡è¦åº“ï¼Œå®ƒæ¶µç›–äº†å¾ˆå¤šè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ¨¡å‹çš„å®ç°ï¼Œå¹¶æä¾›äº†ç®€æ´çš„ API æ¥åŠ è½½ã€å¾®è°ƒå’Œä½¿ç”¨è¿™äº›æ¨¡å‹ã€‚è¿™ä¸ªåº“æ”¯æŒå¤šç§ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€åˆ†ç±»ã€æƒ…æ„Ÿåˆ†æã€ç¿»è¯‘ã€å‘½åå®ä½“è¯†åˆ«ç­‰ã€‚</p>
<h5 id="åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"><a href="#åŠ è½½é¢„è®­ç»ƒæ¨¡å‹" class="headerlink" title="åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"></a>åŠ è½½é¢„è®­ç»ƒæ¨¡å‹</h5><p>transformers åº“çš„æ ¸å¿ƒç‰¹æ€§ä¹‹ä¸€æ˜¯é€šè¿‡ AutoModel å’Œ AutoTokenizer ç±»ï¼Œè½»æ¾åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œå¯¹åº”çš„ tokenizerã€‚AutoModel æ˜¯ä¸€ä¸ªé€šç”¨ç±»ï¼Œå®ƒä¼šæ ¹æ®ä½ æŒ‡å®šçš„æ¨¡å‹åç§°è‡ªåŠ¨åŠ è½½åˆé€‚çš„æ¨¡å‹ã€‚</p>
<p><strong>ä½¿ç”¨æ–¹æ³•</strong>ï¼š</p>
<p>â€¢ å‡è®¾ä½ æƒ³åŠ è½½ä¸€ä¸ªç”¨äºæ–‡æœ¬åˆ†ç±»çš„ BERT æ¨¡å‹ï¼š</p>
<p>from transformers import AutoModelForSequenceClassification, AutoTokenizer</p>
<p>model_name &#x3D; â€œbert-base-uncasedâ€Â  <em># è¿™é‡Œæ˜¯æ¨¡å‹çš„åç§°</em></p>
<p>model &#x3D; AutoModelForSequenceClassification.from_pretrained(model_name)</p>
<p>tokenizer &#x3D; AutoTokenizer.from_pretrained(model_name)</p>
<p>â€¢ AutoModelForSequenceClassification ä¼šæ ¹æ®æ¨¡å‹åç§°ä¸‹è½½é€‚åˆæ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„æ¨¡å‹ç»“æ„ã€‚</p>
<p>â€¢ AutoTokenizer ä¼šåŠ è½½é€‚é…æ¨¡å‹çš„ tokenizerï¼ˆåˆ†è¯å™¨ï¼‰ï¼Œå¹¶å‡†å¤‡å¥½å¯¹è¾“å…¥æ–‡æœ¬è¿›è¡Œç¼–ç ã€‚</p>
<p><strong>2.2 è¿›è¡Œæ¨ç†ï¼ˆInferenceï¼‰</strong></p>
<p>æ¨ç†è¿‡ç¨‹æ˜¯æŒ‡å°†ä¸€ä¸ªè¾“å…¥ä¼ å…¥æ¨¡å‹ï¼Œå¾—åˆ°æ¨¡å‹çš„è¾“å‡ºã€‚transformers åº“æä¾›äº†éå¸¸ç®€ä¾¿çš„æ¥å£æ¥è¿›è¡Œæ¨ç†ã€‚</p>
<p><strong>æ­¥éª¤</strong>ï¼š</p>
<p>â€¢ å‡è®¾ä½ æƒ³ä½¿ç”¨ BERT è¿›è¡Œæƒ…æ„Ÿåˆ†æï¼š</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForSequenceClassification, AutoTokenizer</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">model = AutoModelForSequenceClassification.from_pretrained(<span class="string">&quot;bert-base-uncased&quot;</span>)</span><br><span class="line"></span><br><span class="line">tokenizer = AutoTokenizer.from_pretrained(<span class="string">&quot;bert-base-uncased&quot;</span>)</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"><span class="comment"># ç¼–ç è¾“å…¥æ–‡æœ¬</span></span><br><span class="line"></span><br><span class="line">inputs = tokenizer(<span class="string">&quot;I love Hugging Face!&quot;</span>, return_tensors=<span class="string">&quot;pt&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¨ç†</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line"></span><br><span class="line">Â  Â  logits = model(**inputs).logits</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">_<span class="comment"># è·å–é¢„æµ‹ç»“æœ_</span></span><br><span class="line"></span><br><span class="line">predicted_class = logits.argmax().item()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Predicted class: <span class="subst">&#123;predicted_class&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<p>â€¢ åœ¨è¿™æ®µä»£ç ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆé€šè¿‡ tokenizer ç¼–ç äº†è¾“å…¥æ–‡æœ¬ I love Hugging Face!ï¼Œç„¶åå°†å…¶ä¼ é€’ç»™æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œæœ€åä»æ¨¡å‹è¾“å‡ºä¸­è·å–é¢„æµ‹çš„ç±»æ ‡ç­¾ã€‚</p>
<p><strong>2.3 Fine-tuningï¼ˆå¾®è°ƒï¼‰</strong></p>
<p>å¾®è°ƒæ˜¯æŒ‡å°†é¢„è®­ç»ƒæ¨¡å‹æ ¹æ®è‡ªå·±çš„ä»»åŠ¡å’Œæ•°æ®è¿›è¡Œè¿›ä¸€æ­¥è®­ç»ƒã€‚åœ¨ transformers ä¸­ï¼Œå¾®è°ƒè¿‡ç¨‹å¯ä»¥é€šè¿‡ Trainer API ç®€åŒ–ã€‚Trainer æä¾›äº†å¾ˆå¤šåŠŸèƒ½ï¼ŒåŒ…æ‹¬è®­ç»ƒè¿‡ç¨‹ç®¡ç†ã€è¯„ä¼°ã€æ—¥å¿—è®°å½•ç­‰ã€‚</p>
<p><strong>æ­¥éª¤</strong>ï¼š</p>
<p>â€¢ å‡è®¾ä½ æœ‰ä¸€ä¸ªåˆ†ç±»ä»»åŠ¡æ•°æ®é›†ï¼Œä½ å¯ä»¥ä½¿ç”¨ Trainer æ¥å¾®è°ƒ BERT æ¨¡å‹ï¼š</p>
<p>from transformers import Trainer, TrainingArguments</p>
<p><em># è®¾ç½®è®­ç»ƒå‚æ•°</em></p>
<p>training_args &#x3D; TrainingArguments(</p>
<p>Â  Â  output_dir&#x3D;â€.&#x2F;resultsâ€,Â  Â  Â  Â  Â  <em># æ¨¡å‹è¾“å‡ºæ–‡ä»¶å¤¹</em></p>
<p>Â  Â  evaluation_strategy&#x3D;â€epochâ€, Â  Â  <em># æ¯ä¸ª epoch åè¯„ä¼°</em></p>
<p>Â  Â  per_device_train_batch_size&#x3D;8, Â  <em># æ¯ä¸ªè®¾å¤‡çš„è®­ç»ƒæ‰¹æ¬¡å¤§å°</em></p>
<p>Â  Â  per_device_eval_batch_size&#x3D;8,Â  Â  <em># æ¯ä¸ªè®¾å¤‡çš„è¯„ä¼°æ‰¹æ¬¡å¤§å°</em></p>
<p>Â  Â  logging_dir&#x3D;â€.&#x2F;logsâ€,Â  Â  Â  Â  Â  Â  <em># æ—¥å¿—æ–‡ä»¶å¤¹</em></p>
<p>)</p>
<p>trainer &#x3D; Trainer(</p>
<p>Â  Â  model&#x3D;model, Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <em># ä½ çš„æ¨¡å‹</em></p>
<p>Â  Â  args&#x3D;training_args,Â  Â  Â  Â  Â  Â  Â  Â  <em># è®­ç»ƒå‚æ•°</em></p>
<p>Â  Â  train_dataset&#x3D;train_dataset, Â  Â  Â  <em># è®­ç»ƒæ•°æ®é›†</em></p>
<p>Â  Â  eval_dataset&#x3D;eval_dataset, Â  Â  Â  Â  <em># éªŒè¯æ•°æ®é›†</em></p>
<p>)</p>
<p>trainer.train()</p>
<p>â€¢ åœ¨è¿™ä¸ªä»£ç ä¸­ï¼ŒTrainer ä¼šç®¡ç†æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ï¼ŒåŒ…æ‹¬ä»æ•°æ®åŠ è½½åˆ°æ¨¡å‹è®­ç»ƒï¼Œå¹¶åœ¨æ¯ä¸ª epoch åè¿›è¡Œè¯„ä¼°ã€‚ä½ åªéœ€è¦æŒ‡å®šè®­ç»ƒå‚æ•°å’Œæ•°æ®é›†ã€‚</p>
<p>å¥½çš„ï¼æˆ‘ä»¬ç»§ç»­æ·±å…¥ä»‹ç» Hugging Face çš„å…¶ä»–ä¸»è¦åŠŸèƒ½ï¼ŒåŒ…æ‹¬ <strong>Datasets åº“</strong>ã€<strong>Tokenizers åº“</strong>ã€<strong>Inference API</strong> ç­‰ã€‚</p>
<p><strong>3. Datasets åº“</strong></p>
<p>Hugging Face çš„ datasets åº“æä¾›äº†ä¸€ä¸ªç®€æ´çš„æ¥å£æ¥è®¿é—®å’Œå¤„ç†å„ç§å…¬å…±æ•°æ®é›†ï¼Œå¸®åŠ©å¼€å‘è€…å¿«é€ŸåŠ è½½å’Œä½¿ç”¨æ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œè¯„ä¼°ã€‚è¿™ä¸ªåº“åŒ…æ‹¬äº†è®¸å¤šå¸¸ç”¨çš„ NLP æ•°æ®é›†ï¼Œæ¯”å¦‚ GLUEã€SQuADã€IMDB ç­‰ï¼Œä¹Ÿæ”¯æŒç”¨æˆ·ä¸Šä¼ è‡ªå·±çš„æ•°æ®é›†ã€‚</p>
<p><strong>3.1 åŠ è½½å…¬å…±æ•°æ®é›†</strong></p>
<p>ä½ å¯ä»¥é€šè¿‡ load_dataset å‡½æ•°ç›´æ¥åŠ è½½ Hugging Face Hub ä¸Šçš„å„ç§æ•°æ®é›†ã€‚è¿™ä¸ªåº“æ”¯æŒå¾ˆå¤šæ ¼å¼çš„æ•°æ®é›†ï¼Œæ¯”å¦‚ CSVã€JSONã€æ–‡æœ¬æ–‡ä»¶ç­‰ï¼Œä¹Ÿæ”¯æŒæ ¹æ®ä»»åŠ¡è¿›è¡Œé¢„å¤„ç†ã€‚</p>
<p><strong>ä½¿ç”¨æ–¹æ³•</strong>ï¼š</p>
<p>â€¢ å‡è®¾ä½ è¦åŠ è½½ IMDB æ•°æ®é›†ï¼š</p>
<p>from datasets import load_dataset</p>
<p>dataset &#x3D; load_dataset(â€œimdbâ€)</p>
<p>print(dataset)</p>
<p>è¿™æ®µä»£ç ä¼šè‡ªåŠ¨ä» Hugging Face Hub ä¸‹è½½å¹¶åŠ è½½ IMDB æ•°æ®é›†ã€‚load_dataset ä¼šè¿”å›ä¸€ä¸ªå­—å…¸å¯¹è±¡ï¼ŒåŒ…å«äº†è®­ç»ƒé›†ã€éªŒè¯é›†ç­‰ä¸åŒçš„åˆ†åŒºã€‚</p>
<p>â€¢ ä½ å¯ä»¥æŸ¥çœ‹æ•°æ®é›†çš„å†…å®¹ï¼š</p>
<p>print(dataset[â€˜trainâ€™][0])Â  <em># æŸ¥çœ‹è®­ç»ƒé›†çš„ç¬¬ä¸€ä¸ªæ ·æœ¬</em></p>
<p><strong>3.2 åŠ è½½è‡ªå®šä¹‰æ•°æ®é›†</strong></p>
<p>å¦‚æœä½ æœ‰è‡ªå·±çš„æ•°æ®é›†ï¼Œæ¯”å¦‚ CSV æ–‡ä»¶ï¼Œå¯ä»¥é€šè¿‡ Dataset ç±»å°†å…¶è½¬æ¢ä¸º Hugging Face æ ¼å¼ï¼Œä»¥ä¾¿äºå¤„ç†å’Œä½¿ç”¨ã€‚</p>
<p><strong>ä½¿ç”¨æ–¹æ³•</strong>ï¼š</p>
<p>â€¢ å‡è®¾ä½ æœ‰ä¸€ä¸ªåŒ…å«æ–‡æœ¬å’Œæ ‡ç­¾çš„ CSV æ–‡ä»¶ï¼Œä½ å¯ä»¥å°†å…¶åŠ è½½ä¸º Hugging Face æ•°æ®é›†ï¼š</p>
<p>from datasets import Dataset</p>
<p>import pandas as pd</p>
<p><em># åŠ è½½ CSV æ–‡ä»¶ä¸º pandas DataFrame</em></p>
<p>df &#x3D; pd.read_csv(â€œyour_dataset.csvâ€)</p>
<p><em># å°† DataFrame è½¬æ¢ä¸º Hugging Face Dataset æ ¼å¼</em></p>
<p>dataset &#x3D; Dataset.from_pandas(df)</p>
<p>print(dataset)</p>
<p>â€¢ å¦‚æœä½ çš„æ•°æ®é›†æ˜¯å­˜å‚¨åœ¨ JSON æ–‡ä»¶ä¸­ï¼Œä¹Ÿå¯ä»¥é€šè¿‡ç±»ä¼¼çš„æ–¹æ³•åŠ è½½ï¼š</p>
<p>dataset &#x3D; load_dataset(â€œjsonâ€, data_files&#x3D;â€your_dataset.jsonâ€)</p>
<p><strong>3.3 æ•°æ®é›†æ“ä½œ</strong></p>
<p>Hugging Face çš„ datasets åº“æ”¯æŒå¯¹æ•°æ®é›†è¿›è¡Œå„ç§æ“ä½œï¼Œå¦‚ç­›é€‰ã€æ‹†åˆ†ã€æ‰¹å¤„ç†ç­‰ã€‚</p>
<p>â€¢ <strong>æ•°æ®é›†æ‹†åˆ†</strong>ï¼š</p>
<p>å¦‚æœä½ å¸Œæœ›å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼Œå¯ä»¥ä½¿ç”¨ train_test_split æ–¹æ³•ï¼š</p>
<p>dataset &#x3D; load_dataset(â€œimdbâ€)</p>
<p>train_test &#x3D; dataset[â€œtrainâ€].train_test_split(test_size&#x3D;0.2)</p>
<p>print(train_test)</p>
<p>â€¢ <strong>è¿‡æ»¤æ•°æ®</strong>ï¼š</p>
<p>å¦‚æœä½ æƒ³åŸºäºç‰¹å®šæ¡ä»¶ç­›é€‰æ•°æ®é›†ä¸­çš„æ ·æœ¬ï¼š</p>
<p>dataset_filtered &#x3D; dataset.filter(lambda example: example[â€˜labelâ€™] &#x3D;&#x3D; 1)</p>
<p>print(dataset_filtered)</p>
<p>â€¢ <strong>æ˜ å°„å‡½æ•°</strong>ï¼š</p>
<p>ä½ å¯ä»¥å¯¹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†æ“ä½œï¼Œæ¯”å¦‚æ–‡æœ¬æ¸…æ´—æˆ–åˆ†è¯ï¼š</p>
<p>def preprocess_function(examples):</p>
<p>Â  Â  return tokenizer(examples[â€œtextâ€], padding&#x3D;â€max_lengthâ€, truncation&#x3D;True)</p>
<p>dataset &#x3D; dataset.map(preprocess_function, batched&#x3D;True)</p>
<p><strong>3.4 ä¿å­˜å’Œå…±äº«æ•°æ®é›†</strong></p>
<p>ä½ å¯ä»¥å°†å¤„ç†å¥½çš„æ•°æ®é›†ä¿å­˜ä¸ºæœ¬åœ°æ–‡ä»¶ï¼Œå¹¶ä¸Šä¼ åˆ° Hugging Face Hub è¿›è¡Œåˆ†äº«ã€‚</p>
<p><strong>ä¿å­˜ä¸º CSV æ–‡ä»¶</strong>ï¼š</p>
<p>dataset.to_csv(â€œprocessed_dataset.csvâ€)</p>
<p><strong>ä¸Šä¼ åˆ° Hugging Face Hub</strong>ï¼š</p>
<p>â€¢ åœ¨ Hugging Face Hub ä¸Šåˆ›å»ºä¸€ä¸ªæ–°çš„æ•°æ®é›†é¡µé¢å¹¶ä¸Šä¼ è‡ªå·±çš„æ•°æ®é›†ã€‚</p>
<p><strong>4. Tokenizers åº“</strong></p>
<p>Hugging Face çš„ tokenizers åº“æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„å·¥å…·ï¼Œç”¨äºå¤„ç†æ–‡æœ¬çš„åˆ†è¯å’Œç¼–ç ã€‚å®ƒæ”¯æŒå„ç§åˆ†è¯æ–¹æ³•ï¼Œå¦‚ BPEï¼ˆByte Pair Encodingï¼‰ã€WordPieceã€SentencePiece ç­‰ï¼Œå¯ä»¥å¸®åŠ©ä½ é«˜æ•ˆåœ°å¤„ç†æ–‡æœ¬æ•°æ®ã€‚</p>
<p><strong>4.1 åŠ è½½å’Œä½¿ç”¨ Tokenizer</strong></p>
<p>Hugging Face æä¾›äº†å¤šç§é¢„è®­ç»ƒçš„ tokenizerï¼Œä½ å¯ä»¥ä½¿ç”¨ AutoTokenizer ç±»æ¥åŠ è½½é€‚é…ä¸åŒæ¨¡å‹çš„åˆ†è¯å™¨ã€‚</p>
<p><strong>ä½¿ç”¨æ–¹æ³•</strong>ï¼š</p>
<p>â€¢ å‡è®¾ä½ æƒ³ä½¿ç”¨ BERT æ¨¡å‹çš„ tokenizerï¼š</p>
<p>from transformers import AutoTokenizer</p>
<p>tokenizer &#x3D; AutoTokenizer.from_pretrained(â€œbert-base-uncasedâ€)</p>
<p>encoding &#x3D; tokenizer(â€œHello, Hugging Face!â€)</p>
<p>print(encoding)</p>
<p>è¿™ä¼šå°†è¾“å…¥æ–‡æœ¬ â€œHello, Hugging Face!â€ è½¬æ¢ä¸ºæ¨¡å‹èƒ½å¤Ÿç†è§£çš„ token æ ¼å¼ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªåŒ…å« token IDs çš„å­—å…¸ã€‚</p>
<p><strong>4.2 è‡ªå®šä¹‰ Tokenizer</strong></p>
<p>å¦‚æœä½ æƒ³è‡ªå®šä¹‰ä¸€ä¸ª tokenizerï¼Œå¯ä»¥ä½¿ç”¨ tokenizers åº“æä¾›çš„ä½çº§ APIã€‚ä½ å¯ä»¥é€‰æ‹©ä¸åŒçš„åˆ†è¯ç®—æ³•ï¼ˆå¦‚ BPEã€WordPiece ç­‰ï¼‰ï¼Œå¹¶æ ¹æ®è‡ªå·±çš„è¯­æ–™åº“è®­ç»ƒä¸€ä¸ªæ–°çš„ tokenizerã€‚</p>
<p><strong>åˆ›å»ºè‡ªå®šä¹‰ BPE Tokenizer</strong>ï¼š</p>
<p>from tokenizers import Tokenizer, models, pre_tokenizers, trainers</p>
<p><em># åˆ›å»ºä¸€ä¸ªç©ºçš„ BPE æ¨¡å‹</em></p>
<p>tokenizer &#x3D; Tokenizer(models.BPE())</p>
<p><em># è®¾ç½®åˆ†è¯å™¨çš„é¢„å¤„ç†æ­¥éª¤</em></p>
<p>tokenizer.pre_tokenizer &#x3D; pre_tokenizers.Whitespace()</p>
<p><em># åˆ›å»ºä¸€ä¸ªè®­ç»ƒå™¨ï¼Œè®¾ç½®ç‰¹æ®Š token</em></p>
<p>trainer &#x3D; trainers.BpeTrainer(special_tokens&#x3D;[â€œ[UNK]â€, â€œ[CLS]â€, â€œ[SEP]â€])</p>
<p><em># è®­ç»ƒ tokenizer</em></p>
<p>tokenizer.train_from_file(â€œyour_data.txtâ€, trainer&#x3D;trainer)</p>
<p><em># ä¿å­˜æ¨¡å‹</em></p>
<p>tokenizer.save(â€œcustom_tokenizer.jsonâ€)</p>
<p>â€¢ è¿™æ®µä»£ç ä¼šä½¿ç”¨ä½ çš„æ•°æ®ï¼ˆyour_data.txtï¼‰è®­ç»ƒä¸€ä¸ª BPE åˆ†è¯å™¨ï¼Œå¹¶ä¿å­˜ä¸º custom_tokenizer.json æ–‡ä»¶ã€‚</p>
<p><strong>4.3 Tokenizer çš„é«˜çº§åŠŸèƒ½</strong></p>
<p>â€¢ <strong>åˆ†è¯</strong>ï¼š</p>
<p>tokens &#x3D; tokenizer.encode(â€œHello, Hugging Face!â€)</p>
<p>print(tokens.tokens)Â  <em># è¾“å‡º token åˆ—è¡¨</em></p>
<p>â€¢ <strong>è§£ç </strong>ï¼š</p>
<p>decoded_text &#x3D; tokenizer.decode(tokens.ids)</p>
<p>print(decoded_text)Â  <em># è¾“å‡ºåŸå§‹æ–‡æœ¬</em></p>
<p>â€¢ <strong>ç¼–ç å’Œè§£ç æ‰¹é‡æ•°æ®</strong>ï¼š</p>
<p>ä½ å¯ä»¥å¯¹å¤šä¸ªæ–‡æœ¬æ ·æœ¬è¿›è¡Œæ‰¹é‡ç¼–ç å’Œè§£ç ï¼š</p>
<p>texts &#x3D; [â€œHello, Hugging Face!â€, â€œI love transformers!â€]</p>
<p>encodings &#x3D; tokenizer(texts, padding&#x3D;True, truncation&#x3D;True, return_tensors&#x3D;â€ptâ€)</p>
<p>print(encodings)</p>
<p><strong>5. Inference API (æ¨ç† API)</strong></p>
<p>Hugging Face æä¾›äº†ä¸€ä¸ªå¼ºå¤§çš„æ¨ç† APIï¼Œç”¨æˆ·å¯ä»¥é€šè¿‡ç®€å•çš„ HTTP è¯·æ±‚å°†æ•°æ®ä¼ ç»™åœ¨çº¿æ¨¡å‹ï¼Œè·å–æ¨ç†ç»“æœã€‚è¿™å¯¹äºæ²¡æœ‰è‡ªå·±è®­ç»ƒæ¨¡å‹çš„ç”¨æˆ·ï¼Œæˆ–è€…å¸Œæœ›å¿«é€Ÿéƒ¨ç½²æ¨¡å‹çš„ç”¨æˆ·éå¸¸æœ‰ç”¨ã€‚</p>
<p><strong>5.1 é€šè¿‡ Transformers åº“ä½¿ç”¨æ¨ç† API</strong></p>
<p>Hugging Face æä¾›äº†ä¸€ä¸ªç®€ä¾¿çš„ pipeline APIï¼Œç”¨äºå¿«é€Ÿè¿›è¡Œæ¨ç†ã€‚ä½ åªéœ€è¦ä¼ å…¥æ¨¡å‹çš„åç§°å’Œä»»åŠ¡ç±»å‹ï¼Œpipeline ä¼šè‡ªåŠ¨é€‰æ‹©åˆé€‚çš„æ¨¡å‹å’Œ tokenizerï¼Œå¹¶è¿›è¡Œæ¨ç†ã€‚</p>
<p><strong>ä½¿ç”¨æ–¹æ³•</strong>ï¼š</p>
<p>â€¢ å‡è®¾ä½ æƒ³è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼š</p>
<p>from transformers import pipeline</p>
<p>generator &#x3D; pipeline(â€œtext-generationâ€, model&#x3D;â€gpt2â€)</p>
<p>result &#x3D; generator(â€œOnce upon a timeâ€, max_length&#x3D;50)</p>
<p>print(result)</p>
<p>â€¢ ä½ å¯ä»¥æŒ‡å®šå…¶ä»–ä»»åŠ¡ï¼Œå¦‚æ–‡æœ¬åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ç­‰ï¼š</p>
<p>classifier &#x3D; pipeline(â€œsentiment-analysisâ€)</p>
<p>print(classifier(â€œI love Hugging Face!â€))</p>
<p><strong>5.2 é€šè¿‡ HTTP è°ƒç”¨æ¨ç† API</strong></p>
<p>å¦‚æœä½ ä¸æƒ³ç›´æ¥ä½¿ç”¨ transformers åº“ï¼Œä¹Ÿå¯ä»¥é€šè¿‡ HTTP è¯·æ±‚ç›´æ¥è°ƒç”¨ Hugging Face æä¾›çš„ APIã€‚è¿™å¯¹äºå¼€å‘ Web æœåŠ¡æˆ–é›†æˆå…¶ä»–ç³»ç»Ÿéå¸¸æœ‰ç”¨ã€‚</p>
<p><strong>æ­¥éª¤</strong>ï¼š</p>
<ol>
<li><p>åœ¨ Hugging Face Hub ä¸Šæ‰¾åˆ°ä½ éœ€è¦çš„æ¨¡å‹ï¼Œå¹¶è·å– API Tokenã€‚</p>
</li>
<li><p>ä½¿ç”¨ HTTP è¯·æ±‚è°ƒç”¨æ¨¡å‹ APIã€‚</p>
</li>
</ol>
<p><strong>ç¤ºä¾‹</strong>ï¼š</p>
<p>â€¢ ä½¿ç”¨ curl è°ƒç”¨æ¨ç† APIï¼š</p>
<p>curl -X POST <a target="_blank" rel="noopener" href="https://api-inference.huggingface.co/models/gpt2">https://api-inference.huggingface.co/models/gpt2</a> \</p>
<p>Â Â  Â  -H â€œAuthorization: Bearer YOUR_API_TOKENâ€ \</p>
<p>Â Â  Â  -d â€˜{â€œinputsâ€: â€œOnce upon a timeâ€}â€™</p>
<p>â€¢ ä½ ä¹Ÿå¯ä»¥åœ¨ Python ä¸­ä½¿ç”¨ requests å‘é€ POST è¯·æ±‚ï¼š</p>
<p>import requests</p>
<p>headers &#x3D; {â€œAuthorizationâ€: â€œBearer YOUR_API_TOKENâ€}</p>
<p>data &#x3D; {â€œinputsâ€: â€œOnce upon a timeâ€}</p>
<p>response &#x3D; requests.post(â€œ<a target="_blank" rel="noopener" href="https://api-inference.huggingface.co/models/gpt2">https://api-inference.huggingface.co/models/gpt2</a>â€œ, headers&#x3D;headers, json&#x3D;data)</p>
<p>print(response.json())</p>
<p><strong>5.3 æ¨ç† API çš„é«˜çº§åŠŸèƒ½</strong></p>
<p>â€¢ <strong>æ‰¹é‡æ¨ç†</strong>ï¼šä½ å¯ä»¥ä¸€æ¬¡æ€§å‘é€å¤šä¸ªè¾“å…¥è¿›è¡Œæ‰¹é‡æ¨ç†ã€‚</p>
<p>â€¢ <strong>è‡ªå®šä¹‰å‚æ•°</strong>ï¼šä½ å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´æ¨ç†å‚æ•°ï¼Œä¾‹å¦‚ç”Ÿæˆçš„æ–‡æœ¬é•¿åº¦ã€æ¸©åº¦ã€top-k é‡‡æ ·ç­‰ã€‚</p>
<p><strong>6. Accelerate åº“</strong></p>
<p>accelerate æ˜¯ Hugging Face æä¾›çš„ä¸€ä¸ªç”¨äºåŠ é€Ÿæ¨¡å‹è®­ç»ƒå’Œæ¨ç†çš„åº“ï¼Œç‰¹åˆ«é€‚ç”¨äºå¤§è§„æ¨¡è®­ç»ƒä»»åŠ¡ã€‚å®ƒç®€åŒ–äº†åœ¨å¤šè®¾å¤‡ï¼ˆå¦‚ GPU å’Œ TPUï¼‰ä¸Šå¹¶è¡Œè®­ç»ƒå’Œåˆ†å¸ƒå¼è®­ç»ƒçš„å¤æ‚æ€§ã€‚ä½ å¯ä»¥é€šè¿‡å®ƒæ¥è½»æ¾åœ°åœ¨å¤šä¸ªè®¾å¤‡ä¹‹é—´åˆ†é…ä»»åŠ¡ï¼Œæé«˜è®¡ç®—æ•ˆç‡ã€‚</p>
<p><strong>6.1 å®‰è£… Accelerate</strong></p>
<p>é¦–å…ˆï¼Œä½ éœ€è¦å®‰è£… accelerate åº“ï¼š</p>
<p>pip install accelerate</p>
<p><strong>6.2 ç®€åŒ–å¤šè®¾å¤‡è®­ç»ƒ</strong></p>
<p>accelerate å¯ä»¥å¸®åŠ©ä½ è‡ªåŠ¨ç®¡ç†å¤š GPU æˆ–å¤šæœºå™¨è®­ç»ƒï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡ç®€å•çš„å‘½ä»¤è¿è¡Œæ¨¡å‹è®­ç»ƒã€‚å®ƒæ”¯æŒå¤šç§è®¾å¤‡ç±»å‹ï¼ŒåŒ…æ‹¬ CPUã€GPU å’Œ TPUã€‚</p>
<p><strong>æ­¥éª¤</strong>ï¼š</p>
<ol>
<li>åœ¨è®­ç»ƒè„šæœ¬ä¸­ï¼Œé¦–å…ˆå¯¼å…¥ Accelerator ç±»ï¼š</li>
</ol>
<p>from accelerate import Accelerator</p>
<ol start="2">
<li>åˆ›å»º Accelerator å®ä¾‹å¹¶è®¾ç½®è®­ç»ƒè¿‡ç¨‹ï¼š</li>
</ol>
<p>accelerator &#x3D; Accelerator()</p>
<ol start="3">
<li>ä½¿ç”¨ accelerator.prepare() æ¥è‡ªåŠ¨å‡†å¤‡æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œæ•°æ®åŠ è½½å™¨ï¼š</li>
</ol>
<p>model, optimizer, train_dataloader &#x3D; accelerator.prepare(model, optimizer, train_dataloader)</p>
<ol start="4">
<li>ç„¶åï¼Œä½ å¯ä»¥åƒå¹³å¸¸ä¸€æ ·è¿›è¡Œè®­ç»ƒï¼Œaccelerate ä¼šè‡ªåŠ¨å¤„ç†å¤šè®¾å¤‡çš„åŒæ­¥å’Œè°ƒåº¦ã€‚</li>
</ol>
<p><strong>ç¤ºä¾‹</strong>ï¼šä½¿ç”¨ accelerate è¿›è¡Œç®€å•çš„è®­ç»ƒ</p>
<p>from accelerate import Accelerator</p>
<p>from transformers import AutoModelForSequenceClassification, AutoTokenizer, AdamW</p>
<p>from datasets import load_dataset</p>
<p><em># åˆå§‹åŒ– Accelerator</em></p>
<p>accelerator &#x3D; Accelerator()</p>
<p><em># åŠ è½½æ•°æ®é›†å’Œæ¨¡å‹</em></p>
<p>dataset &#x3D; load_dataset(â€œimdbâ€)</p>
<p>model &#x3D; AutoModelForSequenceClassification.from_pretrained(â€œbert-base-uncasedâ€)</p>
<p>tokenizer &#x3D; AutoTokenizer.from_pretrained(â€œbert-base-uncasedâ€)</p>
<p><em># å‡†å¤‡æ•°æ®</em></p>
<p>train_dataset &#x3D; dataset[â€œtrainâ€].map(lambda x: tokenizer(x[â€œtextâ€], padding&#x3D;True, truncation&#x3D;True), batched&#x3D;True)</p>
<p>train_dataloader &#x3D; torch.utils.data.DataLoader(train_dataset, batch_size&#x3D;8)</p>
<p><em># åˆå§‹åŒ–ä¼˜åŒ–å™¨</em></p>
<p>optimizer &#x3D; AdamW(model.parameters(), lr&#x3D;5e-5)</p>
<p><em># å‡†å¤‡æ‰€æœ‰å†…å®¹</em></p>
<p>model, optimizer, train_dataloader &#x3D; accelerator.prepare(model, optimizer, train_dataloader)</p>
<p><em># è®­ç»ƒè¿‡ç¨‹</em></p>
<p>for epoch in range(3):</p>
<p>Â  Â  model.train()</p>
<p>Â  Â  for batch in train_dataloader:</p>
<p>Â  Â  Â  Â  optimizer.zero_grad()</p>
<p>Â  Â  Â  Â  inputs &#x3D; {key: batch[key].to(accelerator.device) for key in batch}</p>
<p>Â  Â  Â  Â  labels &#x3D; inputs.pop(â€œlabelâ€)</p>
<p>Â  Â  Â  Â  outputs &#x3D; model(**inputs, labels&#x3D;labels)</p>
<p>Â  Â  Â  Â  loss &#x3D; outputs.loss</p>
<p>Â  Â  Â  Â  accelerator.backward(loss)</p>
<p>Â  Â  Â  Â  optimizer.step()</p>
<p>Â  Â  print(fâ€Epoch {epoch} completedâ€)</p>
<p>é€šè¿‡ä½¿ç”¨ accelerateï¼Œä½ å¯ä»¥è½»æ¾åœ°åœ¨å¤šè®¾å¤‡ä¸Šè¿›è¡Œè®­ç»ƒï¼Œä¸éœ€è¦æ‰‹åŠ¨ç¼–å†™å¤æ‚çš„åˆ†å¸ƒå¼è®­ç»ƒä»£ç ã€‚accelerate å¤„ç†äº†æ•°æ®å¹¶è¡Œã€æ¢¯åº¦ç´¯ç§¯ç­‰æ–¹é¢çš„å†…å®¹ã€‚</p>
<p><strong>6.3 å¹¶è¡Œæ¨ç†</strong></p>
<p>accelerate è¿˜æ”¯æŒæ¨ç†æ—¶çš„å¤šè®¾å¤‡åŠ é€Ÿã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æœ‰å¤šä¸ª GPUï¼Œå¹¶å¸Œæœ›å°†æ¨ç†è´Ÿè½½åˆ†é…åˆ°å¤šä¸ªè®¾å¤‡ä¸Šï¼Œå¯ä»¥ä½¿ç”¨ accelerate æ¥åŠ é€Ÿæ¨ç†ã€‚</p>
<p>from accelerate import Accelerator</p>
<p>from transformers import AutoModelForSequenceClassification, AutoTokenizer</p>
<p>accelerator &#x3D; Accelerator()</p>
<p>model &#x3D; AutoModelForSequenceClassification.from_pretrained(â€œbert-base-uncasedâ€)</p>
<p>tokenizer &#x3D; AutoTokenizer.from_pretrained(â€œbert-base-uncasedâ€)</p>
<p><em># å‡†å¤‡æ¨¡å‹å’Œ tokenizer</em></p>
<p>model, tokenizer &#x3D; accelerator.prepare(model, tokenizer)</p>
<p><em># æ¨ç†</em></p>
<p>inputs &#x3D; tokenizer(â€œHello, Hugging Face!â€, return_tensors&#x3D;â€ptâ€)</p>
<p>inputs &#x3D; {key: value.to(accelerator.device) for key, value in inputs.items()}</p>
<p>outputs &#x3D; model(**inputs)</p>
<p><strong>7. Hugging Face Spaces</strong></p>
<p>Hugging Face Spaces æ˜¯ä¸€ä¸ªç”¨äºæ‰˜ç®¡å’Œåˆ†äº«æœºå™¨å­¦ä¹ æ¨¡å‹å’Œåº”ç”¨çš„å¹³å°ã€‚ä½ å¯ä»¥åœ¨ Hugging Face Spaces ä¸Šåˆ›å»ºã€éƒ¨ç½²å’Œåˆ†äº«ä½ çš„æœºå™¨å­¦ä¹ åº”ç”¨ï¼Œæ”¯æŒé€šè¿‡ Streamlitã€Gradio ç­‰åº“å¿«é€Ÿæ­å»ºäº¤äº’å¼ç•Œé¢ã€‚</p>
<p><strong>7.1 åˆ›å»º Hugging Face Spaces</strong></p>
<p>Hugging Face Spaces å…è®¸ç”¨æˆ·é€šè¿‡ç®€å•çš„ä»£ç å’Œç•Œé¢ï¼Œå¿«é€Ÿæ„å»º Web åº”ç”¨æ¥å±•ç¤ºæ¨¡å‹çš„æ¨ç†èƒ½åŠ›ã€‚ä½ å¯ä»¥é€‰æ‹©ä½¿ç”¨ Gradio æˆ– Streamlit æ¥æ„å»ºåº”ç”¨ç•Œé¢ã€‚</p>
<p><strong>æ­¥éª¤</strong>ï¼š</p>
<ol>
<li><p>ç™»å½• Hugging Face è´¦æˆ·å¹¶åˆ›å»ºä¸€ä¸ª Space é¡µé¢ã€‚</p>
</li>
<li><p>åœ¨ Space ä¸­åˆ›å»ºä¸€ä¸ªåº”ç”¨ï¼Œå¯ä»¥é€‰æ‹© Gradio æˆ– Streamlit æ¡†æ¶ã€‚</p>
</li>
</ol>
<p><strong>ç¤ºä¾‹ï¼šä½¿ç”¨ Gradio åˆ›å»ºäº¤äº’å¼ç•Œé¢</strong>ï¼š</p>
<p>pip install gradio</p>
<p>ç„¶åï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„ Gradio åº”ç”¨æ¥å±•ç¤ºæ¨¡å‹ï¼š</p>
<p>import gradio as gr</p>
<p>from transformers import pipeline</p>
<p><em># åŠ è½½é¢„è®­ç»ƒæ¨¡å‹</em></p>
<p>classifier &#x3D; pipeline(â€œsentiment-analysisâ€)</p>
<p><em># åˆ›å»º Gradio ç•Œé¢</em></p>
<p>def predict(text):</p>
<p>Â  Â  return classifier(text)</p>
<p><em># è®¾ç½®ç•Œé¢</em></p>
<p>interface &#x3D; gr.Interface(fn&#x3D;predict, inputs&#x3D;â€textâ€, outputs&#x3D;â€jsonâ€)</p>
<p><em># å¯åŠ¨åº”ç”¨</em></p>
<p>interface.launch()</p>
<p>è¿™æ®µä»£ç åˆ›å»ºäº†ä¸€ä¸ªç®€å•çš„æ–‡æœ¬åˆ†ç±»åº”ç”¨ï¼Œç”¨æˆ·å¯ä»¥è¾“å…¥æ–‡æœ¬ï¼Œåº”ç”¨ä¼šè¿”å›è¯¥æ–‡æœ¬çš„æƒ…æ„Ÿåˆ†æç»“æœã€‚</p>
<p><strong>7.2 ä½¿ç”¨ Streamlit åˆ›å»ºäº¤äº’å¼ç•Œé¢</strong></p>
<p>Streamlit ä¹Ÿæ˜¯ä¸€ä¸ªéå¸¸æµè¡Œçš„ Python åº“ï¼Œç”¨äºå¿«é€Ÿåˆ›å»º Web åº”ç”¨ã€‚ä½ å¯ä»¥åœ¨ Hugging Face Spaces ä¸­ä½¿ç”¨ Streamlit æ„å»ºåº”ç”¨ã€‚</p>
<p>pip install streamlit</p>
<p>ç„¶åï¼Œåˆ›å»ºä¸€ä¸ªç®€å•çš„åº”ç”¨ï¼š</p>
<p>import streamlit as st</p>
<p>from transformers import pipeline</p>
<p><em># åŠ è½½æ¨¡å‹</em></p>
<p>classifier &#x3D; pipeline(â€œsentiment-analysisâ€)</p>
<p><em># åˆ›å»º Streamlit åº”ç”¨</em></p>
<p>st.title(â€œSentiment Analysisâ€)</p>
<p>text_input &#x3D; st.text_area(â€œEnter text:â€)</p>
<p>if text_input:</p>
<p>Â  Â  result &#x3D; classifier(text_input)</p>
<p>Â  Â  st.write(result)</p>
<p>ç„¶åï¼Œé€šè¿‡ streamlit run å¯åŠ¨åº”ç”¨ï¼š</p>
<p>streamlit run app.py</p>
<p>è¿™æ ·ä½ å°±å¯ä»¥é€šè¿‡ Streamlit åˆ›å»ºä¸€ä¸ªç®€å•çš„åº”ç”¨ï¼Œç”¨æˆ·è¾“å…¥æ–‡æœ¬åå³å¯çœ‹åˆ°æƒ…æ„Ÿåˆ†æçš„ç»“æœã€‚</p>
<p><strong>7.3 åˆ†äº«å’Œéƒ¨ç½²åº”ç”¨</strong></p>
<p>ä¸€æ—¦ä½ åˆ›å»ºäº† Hugging Face Spaceï¼Œä½ å¯ä»¥åˆ†äº«å®ƒçš„é“¾æ¥ç»™å…¶ä»–äººï¼Œè®©ä»–ä»¬ä¹Ÿèƒ½è®¿é—®å’Œä½¿ç”¨ä½ çš„åº”ç”¨ã€‚Hugging Face ä¼šä¸ºæ¯ä¸ª Space æä¾›ä¸€ä¸ªç‹¬ç«‹çš„ URLï¼Œä½ å¯ä»¥å°†å…¶åµŒå…¥åˆ°æ–‡æ¡£ã€åšå®¢ç­‰åœ°æ–¹ã€‚</p>
<p>â€¢ åœ¨ Space é¡µé¢ä¸Šï¼Œä½ å¯ä»¥é€‰æ‹©å…¬å¼€æˆ–ç§æœ‰ä½ çš„åº”ç”¨ã€‚å¦‚æœä½ é€‰æ‹©å…¬å¼€ï¼Œå…¶ä»–äººä¹Ÿå¯ä»¥è®¿é—®å¹¶ä½¿ç”¨è¿™ä¸ªåº”ç”¨ã€‚</p>
<p>â€¢ ä½ ä¹Ÿå¯ä»¥åœ¨åº”ç”¨ä¸­ä¸Šä¼ è‡ªå·±çš„æ•°æ®æˆ–æ¨¡å‹æ–‡ä»¶ï¼Œä½¿å¾—åº”ç”¨æ›´åŠ ä¸°å¯Œå’Œä¸ªæ€§åŒ–ã€‚</p>
<p><strong>8. Hugging Face æ–‡æ¡£å’Œç¤¾åŒºæ”¯æŒ</strong></p>
<p>Hugging Face æä¾›äº†ä¸°å¯Œçš„æ–‡æ¡£å’Œç¤¾åŒºæ”¯æŒï¼Œå¸®åŠ©å¼€å‘è€…è§£å†³é—®é¢˜å¹¶å­¦ä¹ å¦‚ä½•ä½¿ç”¨ Hugging Face çš„å·¥å…·ã€‚</p>
<p><strong>8.1 å®˜æ–¹æ–‡æ¡£</strong></p>
<p>â€¢ <a target="_blank" rel="noopener" href="https://huggingface.co/docs">Hugging Face å®˜æ–¹æ–‡æ¡£</a>ï¼šåŒ…å«äº†ä»æ¨¡å‹åŠ è½½ã€è®­ç»ƒåˆ°éƒ¨ç½²ç­‰å„ä¸ªæ–¹é¢çš„è¯¦ç»†è¯´æ˜ï¼Œæ¶µç›–äº†å„ç±» API å’ŒåŠŸèƒ½çš„ä½¿ç”¨ç¤ºä¾‹ã€‚</p>
<p><strong>8.2 Hugging Face è®ºå›</strong></p>
<p>â€¢ <a target="_blank" rel="noopener" href="https://discuss.huggingface.co/">Hugging Face è®ºå›</a>ï¼šä½ å¯ä»¥åœ¨è¿™ä¸ªè®ºå›ä¸Šå‘ç¤¾åŒºæé—®ï¼Œåˆ†äº«ç»éªŒï¼Œè®¨è®ºæœ€æ–°çš„ç ”ç©¶æˆæœã€‚è®ºå›ä¸­æœ‰è®¸å¤šæ¥è‡ªä¸åŒé¢†åŸŸçš„ä¸“å®¶ï¼Œä»–ä»¬ä¼šå›ç­”ä½ çš„é—®é¢˜ï¼Œæä¾›å¸®åŠ©ã€‚</p>
<p><strong>8.3 Hugging Face Discord</strong></p>
<p>â€¢ Hugging Face è¿˜æä¾›äº†ä¸€ä¸ª <a target="_blank" rel="noopener" href="https://discord.gg/huggingface">Discord é¢‘é“</a>ï¼Œä½ å¯ä»¥åœ¨è¿™é‡Œä¸å…¶ä»–å¼€å‘è€…å’Œç ”ç©¶äººå‘˜å®æ—¶äº¤æµã€‚</p>
<p><strong>æ€»ç»“</strong></p>
<p>æˆ‘ä»¬å·²ç»è¯¦ç»†æ¢è®¨äº† Hugging Face çš„è®¸å¤šåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š</p>
<ol>
<li><p><strong>Accelerate åº“</strong>ï¼šå¸®åŠ©åœ¨å¤šä¸ªè®¾å¤‡ä¸ŠåŠ é€Ÿè®­ç»ƒå’Œæ¨ç†ï¼Œç®€åŒ–åˆ†å¸ƒå¼è®­ç»ƒã€‚</p>
</li>
<li><p><strong>Hugging Face Spaces</strong>ï¼šç”¨äºåˆ›å»ºå’Œéƒ¨ç½²æœºå™¨å­¦ä¹ åº”ç”¨ï¼Œæ”¯æŒ Gradio å’Œ Streamlit ç•Œé¢ï¼Œæ–¹ä¾¿å±•ç¤ºå’Œåˆ†äº«æ¨¡å‹ã€‚</p>
</li>
<li><p><strong>æ–‡æ¡£å’Œç¤¾åŒºæ”¯æŒ</strong>ï¼šæä¾›ä¸°å¯Œçš„æ–‡æ¡£ã€è®ºå›å’Œå®æ—¶äº¤æµå¹³å°ï¼Œå¸®åŠ©å¼€å‘è€…è§£å†³é—®é¢˜ã€‚</p>
</li>
</ol>
<p>é€šè¿‡ä½¿ç”¨ Hugging Face æä¾›çš„å·¥å…·å’Œå¹³å°ï¼Œä½ å¯ä»¥é«˜æ•ˆåœ°æ„å»ºã€è®­ç»ƒã€éƒ¨ç½²å’Œåˆ†äº«æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚å¸Œæœ›è¿™äº›å†…å®¹èƒ½å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œå¦‚æœæœ‰ä»»ä½•é—®é¢˜æˆ–éœ€è¦è¿›ä¸€æ­¥çš„ç¤ºä¾‹ï¼Œéšæ—¶å‘Šè¯‰æˆ‘ï¼</p>
</p></div><div class="share"><span>Share</span>&nbsp;<span class="soc"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></span><span class="soc"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></span><span class="soc"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=http://example.com/2024/11/28/huggingface/%20Duffy%20hugging face æŠ±æŠ±è„¸"></a></span></div><div class="pagination"><p class="clearfix"><span class="pre pagbuttons"><a role="navigation" href="/2024/12/01/transformer/" title="all you need is all you need"><i class="fa fa-angle-double-left"></i>&nbsp;Previous post: all you need is all you need</a></span><span>&nbsp;</span><span class="next pagbuttons"><a role="navigation" href="/2024/11/13/hot100/" title="åŠ›æ‰£Hot100">Next post: åŠ›æ‰£Hot100&nbsp;<i class="fa fa-angle-double-right"></i></a></span></p></div></div></div></div><div class="visible-xs site-bottom-footer"><footer><p>&copy;&nbsp;2025&nbsp;<a target="_blank" href="http://example.com" rel="noopener noreferrer">Kai</a></p><p>Theme&nbsp;<a target="_blank" href="https://github.com/SumiMakito/hexo-theme-typography" rel="noopener noreferrer">Typography</a>&nbsp;by&nbsp;<a target="_blank" href="https://www.keep.moe" rel="noopener noreferrer">Makito</a></p><p>Proudly published with&nbsp;<a target="_blank" href="https://hexo.io" rel="noopener noreferrer">Hexo</a></p></footer></div></div></div></div><script src="/js/jquery-3.1.0.min.js"></script><script src="/js/bootstrap.min.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/google-analytics.js"></script><script src="/js/typography.js"></script></body></html>