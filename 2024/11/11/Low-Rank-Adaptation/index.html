<!DOCTYPE html><html lang="zh-CN"><head><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><meta name="author"><title>LoRA微调 · Duffy</title><meta name="description" content="现在的大模型动辄几十、上百亿参数，如果我们想要对模型进行微调，放到本地一般而言很少有足够的显存。
大模型微调的几种方法：

低秩微调LoRA

LoRA（Low-Rank Adaptation）简介：高效的深度学习模型微调方法
在深度学习领域，尤其是大规模预训练模型（如BERT、GPT系列等）的微调"><meta name="og:description" content="现在的大模型动辄几十、上百亿参数，如果我们想要对模型进行微调，放到本地一般而言很少有足够的显存。
大模型微调的几种方法：

低秩微调LoRA

LoRA（Low-Rank Adaptation）简介：高效的深度学习模型微调方法
在深度学习领域，尤其是大规模预训练模型（如BERT、GPT系列等）的微调"><meta name="twitter:site" content="Duffy"><meta name="twitter:title" content="LoRA微调"><meta name="twitter:card" content="summary"><meta name="keywords" content=""><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="renderer" content="webkit"><link rel="short icon" href="images/favicon.png" type="image/x-icon"><link rel="stylesheet" href="/css/bootstrap.min.css"><link rel="stylesheet" href="/css/font-awesome.min.css"><link rel="stylesheet" href="/css/style.css"><link rel="alternate" type="application/atom+xml" title="ATOM 1.0" href="/atom.xml"><meta name="generator" content="Hexo 7.3.0"><link rel="stylesheet" href="/css/prism.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head><body><div class="container" id="stage"><div class="row"><div class="col-sm-3 col-xs-12 side-container invisible" id="side-bar"><div class="vertical-text site-title"><h3 class="site-title-small" tabindex="-1"><a class="a-title" href="/">Keep going</a></h3><h1 class="site-title-large" tabindex="-1"><a class="a-title" href="/">思考｜记录</a></h1><!--h6(onclick="triggerSiteNav()") Trigger--></div><br class="visible-lg visible-md visible-sm"><div class="site-title-links" id="site-nav"><ul><li><a href="/">Home</a></li><li><a href="/archives">Archive</a></li><li><a href="/tags">Tags</a></li><li><a href="/about/about.html"></a></li><li><a href="/about/index.html">about</a></li><li><a href="/about/heureux.html"></a></li><li class="soc"><a href="https://github.com/heureux831" target="_blank" rel="noopener noreferrer" aria-label="Github"><i class="fa fa-github">&nbsp;</i></a><a href="http://example.com/atom.xml" target="_blank" rel="noopener noreferrer" aria-label="RSS"><i class="fa fa-rss">&nbsp;</i></a></li></ul><div class="visible-lg visible-md visible-sm site-nav-footer"><br class="site-nav-footer-br"><footer><p>&copy;&nbsp;2025&nbsp;<a target="_blank" href="http://example.com" rel="noopener noreferrer">Kai</a></p><p>Theme&nbsp;<a target="_blank" href="https://github.com/SumiMakito/hexo-theme-typography" rel="noopener noreferrer">Typography</a>&nbsp;by&nbsp;<a target="_blank" href="https://www.keep.moe" rel="noopener noreferrer">Makito</a></p><p>Proudly published with&nbsp;<a target="_blank" href="https://hexo.io" rel="noopener noreferrer">Hexo</a></p></footer></div></div></div><div class="col-sm-9 col-xs-12 main-container invisible" id="main-container"><div class="autopagerize_page_element"><div class="content"><div class="post-page"><div class="post-container"><p class="post-title"><a>LoRA微调</a></p><p class="post-meta"><span class="date meta-item">Posted at&nbsp;2024-11-11</span><span class="meta-item"><i class="fa fa-tag"></i><span>&nbsp;</span><a class="a-tag" href="/tags/LLM/" title="LLM">LLM</a><span>&nbsp;</span><a class="a-tag" href="/tags/finetuning/" title="finetuning">finetuning</a><span>&nbsp;</span></span></p><p class="post-abstract"><p>现在的大模型动辄几十、上百亿参数，如果我们想要对模型进行微调，放到本地一般而言很少有足够的显存。</p>
<p>大模型微调的几种方法：</p>
<ul>
<li>低秩微调LoRA</li>
</ul>
<p><strong>LoRA（Low-Rank Adaptation）简介：高效的深度学习模型微调方法</strong></p>
<p>在深度学习领域，尤其是大规模预训练模型（如BERT、GPT系列等）的微调过程中，传统的微调方法通常需要修改模型的所有参数。这样做虽然可以获得较好的性能，但代价非常高，特别是在面对非常大的预训练模型时。LoRA（Low-Rank Adaptation）作为一种新的微调技术，提出了一种更加高效且参数量更少的方法来进行微调。</p>
<p><strong>什么是LoRA？</strong></p>
<p>LoRA是“低秩适应”技术的缩写，其核心思想是在进行模型微调时，通过将权重矩阵分解为低秩矩阵来减少可调参数的数量。这使得我们在不改变原始模型结构的情况下，能够以更少的计算和存储开销，快速适应下游任务。</p>
<p>具体来说，LoRA通过在神经网络中的每一层加入一个低秩适应矩阵，这些矩阵只会在微调过程中进行更新，而原始的预训练参数保持不变。</p>
<p><strong>LoRA的原理</strong></p>
<p>在传统的神经网络中，参数矩阵通常是一个高维矩阵。例如，在一个线性层中，假设权重矩阵是 ，其中 和 分别是输入和输出的维度。</p>
<p>LoRA的关键想法是将这个高维矩阵 分解为两个低秩矩阵 和 ，即：</p>
<p>其中， 是我们在微调过程中需要学习的低秩适应矩阵。通过这种方式，只有 和 两个矩阵需要更新，且它们的秩通常设置为较小的值（例如，秩为 ）。这样可以显著减少需要训练的参数数量，同时保留原始模型的表示能力。</p>
<p><strong>为什么LoRA有效？</strong></p>
<ol>
<li><p><strong>减少计算开销</strong>：由于LoRA只需要更新较小的低秩矩阵，避免了对整个模型参数的更新，因此计算资源消耗大大降低，尤其是在大规模预训练模型上。</p>
</li>
<li><p><strong>适用于大模型</strong>：大规模预训练模型通常包含数亿甚至数十亿个参数，直接微调这些模型往往需要巨大的计算和存储资源。LoRA通过限制可调参数的数量，使得在有限资源下仍然能够有效地进行微调。</p>
</li>
<li><p><strong>保持预训练知识</strong>：LoRA不会修改原始的预训练权重，只是引入了少量的适应性变化。因此，模型能够保留从大规模数据集学到的知识，同时针对具体任务进行微调。</p>
</li>
</ol>
<p><strong>LoRA的实现</strong></p>
<p>假设我们有一个现成的神经网络模型（例如BERT或GPT），而我们想要进行LoRA微调。LoRA的实现步骤如下：</p>
<ol>
<li><p><strong>选择适合微调的层</strong>：首先，我们需要选择模型中哪些层需要进行LoRA微调。通常会选择自注意力层（Attention）和前馈神经网络（FeedForward）层。</p>
</li>
<li><p><strong>插入低秩适应矩阵</strong>：在选定的层中，我们将原始权重矩阵 替换为 ，其中 和 是低秩矩阵。</p>
</li>
<li><p><strong>训练低秩矩阵</strong>：在微调过程中，我们只更新低秩矩阵 和 ，而保持原始模型权重不变。</p>
</li>
<li><p><strong>评估性能</strong>：在完成微调后，我们评估模型在下游任务上的性能，看看LoRA是否能够有效地提高模型在特定任务上的表现。</p>
</li>
</ol>
<p><strong>LoRA的优点</strong></p>
<p>• <strong>参数效率高</strong>：LoRA只需要更新少量参数，通常比传统微调方法的参数更新量要小得多，尤其是在大规模预训练模型中。</p>
<p>• <strong>节省计算资源</strong>：LoRA的低秩适应矩阵可以显著减少训练和推理时的计算开销。</p>
<p>• <strong>简洁易用</strong>：LoRA的实现相对简单，可以作为一个插件式的微调方法，适用于多种不同的神经网络架构。</p>
<p><strong>LoRA的应用场景</strong></p>
<p>LoRA非常适合用于以下几种场景：</p>
<ol>
<li><p><strong>大规模预训练模型微调</strong>：在处理诸如BERT、GPT等大型模型时，LoRA能够有效减少微调时的计算资源消耗。</p>
</li>
<li><p><strong>低资源环境下的迁移学习</strong>：如果资源有限（例如显存不足），LoRA能够帮助在不牺牲性能的前提下进行迁移学习。</p>
</li>
<li><p><strong>模型压缩</strong>：LoRA也可以用于模型压缩，尤其是在需要在嵌入式设备或移动设备上运行大模型时，LoRA可以通过减少训练参数的数量来降低模型的存储需求。</p>
</li>
</ol>
<p><strong>示例代码</strong></p>
<p>以下是一个简单的Python示例，展示了如何在PyTorch中实现LoRA微调：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LoRALayer</span>(nn.Module):</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_dim, output_dim, rank</span>):</span><br><span class="line"></span><br><span class="line">        <span class="built_in">super</span>(LoRALayer, <span class="variable language_">self</span>).__init__()</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.input_dim = input_dim</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.output_dim = output_dim</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.rank = rank</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">        <span class="comment"># 初始化低秩矩阵 A 和 B</span></span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.A = nn.Parameter(torch.randn(input_dim, rank))</span><br><span class="line"></span><br><span class="line">        <span class="variable language_">self</span>.B = nn.Parameter(torch.randn(rank, output_dim))</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line"></span><br><span class="line">        _<span class="comment"># 计算低秩适应矩阵的贡献_</span></span><br><span class="line"></span><br><span class="line">        delta_W = torch.mm(<span class="variable language_">self</span>.A, <span class="variable language_">self</span>.B)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> torch.matmul(x, delta_W)</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">_<span class="comment"># 模型层的使用示例_</span></span><br><span class="line"></span><br><span class="line">input_dim = <span class="number">768</span>  _<span class="comment"># 输入维度_</span></span><br><span class="line"></span><br><span class="line">output_dim = <span class="number">256</span>  _<span class="comment"># 输出维度_</span></span><br><span class="line"></span><br><span class="line">rank = <span class="number">8</span>  _<span class="comment"># 低秩矩阵的秩_</span></span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line">_<span class="comment"># 创建LoRA层_</span></span><br><span class="line"></span><br><span class="line">lora_layer = LoRALayer(input_dim, output_dim, rank)</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设输入是一个batch的样本_</span></span><br><span class="line"></span><br><span class="line">x = torch.randn(<span class="number">32</span>, input_dim)  _<span class="comment"># 假设batch size为32_</span></span><br><span class="line"><span class="comment"># 前向传播_</span></span><br><span class="line">output = lora_layer(x)</span><br></pre></td></tr></table></figure>
<p><strong>总结</strong></p>
<p>LoRA（Low-Rank Adaptation）作为一种高效的模型微调方法，能够在保持大规模预训练模型强大能力的同时，大幅度减少微调时的计算和存储开销。它通过将模型的权重矩阵分解为低秩矩阵，只更新这些低秩矩阵，而不需要改变原始模型的参数，从而实现了资源高效的迁移学习。</p>
<p>这种方法在处理大规模预训练模型、低资源环境中的迁移学习、以及模型压缩等任务中具有显著的优势。</p>
<p>希望这篇文章对你理解LoRA有帮助！如果有任何问题或需要进一步的示例，欢迎随时提问。</p>
</p></div><div class="share"><span>Share</span>&nbsp;<span class="soc"><a class="fa fa-bookmark" href="javascript:(function(){EN_CLIP_HOST='http://www.evernote.com';try{var%20x=document.createElement('SCRIPT');x.type='text/javascript';x.src=EN_CLIP_HOST+'/public/bookmarkClipper.js?'+(new%20Date().getTime()/100000);document.getElementsByTagName('head')[0].appendChild(x);}catch(e){location.href=EN_CLIP_HOST+'/clip.action?url='+encodeURIComponent(location.href)+'&amp;title='+encodeURIComponent(document.title);}})();" ref="nofollow" target="_blank"></a></span><span class="soc"><a class="fa fa-weibo" href="javascript:void((function(s,d,e){try{}catch(e){}var f='http://service.weibo.com/share/share.php?',u=d.location.href,p=['url=',e(u),'&amp;title=',e(d.title),'&amp;appkey=2924220432'].join('');function a(){if(!window.open([f,p].join(''),'mb',['toolbar=0,status=0,resizable=1,width=620,height=450,left=',(s.width-620)/2,',top=',(s.height-450)/2].join('')))u.href=[f,p].join('');};if(/Firefox/.test(navigator.userAgent)){setTimeout(a,0)}else{a()}})(screen,document,encodeURIComponent));"></a></span><span class="soc"><a class="fa fa-twitter" target="_blank" rel="noopener" href="http://twitter.com/home?status=http://example.com/2024/11/11/Low-Rank-Adaptation/%20Duffy%20LoRA微调"></a></span></div><div class="pagination"><p class="clearfix"><span class="pre pagbuttons"><a role="navigation" href="/2024/11/13/hot100/" title="力扣Hot100"><i class="fa fa-angle-double-left"></i>&nbsp;Previous post: 力扣Hot100</a></span><span>&nbsp;</span><span class="next pagbuttons"><a role="navigation" href="/2024/11/11/dive-into-git/" title="dive-into-git">Next post: dive-into-git&nbsp;<i class="fa fa-angle-double-right"></i></a></span></p></div></div></div></div><div class="visible-xs site-bottom-footer"><footer><p>&copy;&nbsp;2025&nbsp;<a target="_blank" href="http://example.com" rel="noopener noreferrer">Kai</a></p><p>Theme&nbsp;<a target="_blank" href="https://github.com/SumiMakito/hexo-theme-typography" rel="noopener noreferrer">Typography</a>&nbsp;by&nbsp;<a target="_blank" href="https://www.keep.moe" rel="noopener noreferrer">Makito</a></p><p>Proudly published with&nbsp;<a target="_blank" href="https://hexo.io" rel="noopener noreferrer">Hexo</a></p></footer></div></div></div></div><script src="/js/jquery-3.1.0.min.js"></script><script src="/js/bootstrap.min.js"></script><script src="/js/jquery-migrate-1.2.1.min.js"></script><script src="/js/jquery.appear.js"></script><script src="/js/google-analytics.js"></script><script src="/js/typography.js"></script></body></html>